25a26,27
> import static org.easymock.EasyMock.anyObject;
> import static org.easymock.EasyMock.createNiceMock;
27c29,30
< import static org.easymock.EasyMock.isA;
---
> import static org.easymock.EasyMock.replay;
> import static org.easymock.EasyMock.reset;
29c32
< import java.util.Arrays;
---
> import java.util.ArrayList;
34c37
< import java.util.LinkedHashMap;
---
> import java.util.List;
38,40c41,52
< import org.apache.ambari.server.controller.AmbariManagementController;
< import org.apache.ambari.server.controller.StackServiceResponse;
< import org.easymock.EasyMockSupport;
---
> import org.apache.ambari.server.state.ServiceInfo;
> import org.apache.ambari.server.topology.Blueprint;
> import org.apache.ambari.server.topology.Cardinality;
> import org.apache.ambari.server.topology.ClusterTopology;
> import org.apache.ambari.server.topology.ClusterTopologyImpl;
> import org.apache.ambari.server.topology.Configuration;
> import org.apache.ambari.server.topology.HostGroup;
> import org.apache.ambari.server.topology.HostGroupImpl;
> import org.apache.ambari.server.topology.HostGroupInfo;
> import org.apache.ambari.server.topology.InvalidTopologyException;
> import org.junit.After;
> import org.junit.Before;
47a60,148
>   private static final Configuration EMPTY_CONFIG = new Configuration(Collections.<String, Map<String, String>>emptyMap(),
>       Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
>   private final Map<String, Collection<String>> serviceComponents = new HashMap<String, Collection<String>>();
> 
>   private final Blueprint bp = createNiceMock(Blueprint.class);
>   
>   private final ServiceInfo serviceInfo = createNiceMock(ServiceInfo.class);
>   private final Stack stack = createNiceMock(Stack.class);
> 
>   @Before
>   public void init() throws Exception {
>     expect(bp.getStack()).andReturn(stack).anyTimes();
>     expect(bp.getName()).andReturn("test-bp").anyTimes();
> 
>     expect(stack.getName()).andReturn("testStack").anyTimes();
>     expect(stack.getVersion()).andReturn("1").anyTimes();
>     
>     expect(stack.isMasterComponent((String) anyObject())).andReturn(false).anyTimes();
> 
>     expect(serviceInfo.getRequiredProperties()).andReturn(
>         Collections.<String, org.apache.ambari.server.state.PropertyInfo>emptyMap()).anyTimes();
>     expect(serviceInfo.getRequiredServices()).andReturn(Collections.<String>emptyList()).anyTimes();
> 
>     Collection<String> hdfsComponents = new HashSet<String>();
>     hdfsComponents.add("NAMENODE");
>     hdfsComponents.add("SECONDARY_NAMENODE");
>     hdfsComponents.add("DATANODE");
>     hdfsComponents.add("HDFS_CLIENT");
>     serviceComponents.put("HDFS", hdfsComponents);
> 
>     Collection<String> yarnComponents = new HashSet<String>();
>     yarnComponents.add("RESOURCEMANAGER");
>     yarnComponents.add("NODEMANAGER");
>     yarnComponents.add("YARN_CLIENT");
>     yarnComponents.add("APP_TIMELINE_SERVER");
>     serviceComponents.put("YARN", yarnComponents);
> 
>     Collection<String> mrComponents = new HashSet<String>();
>     mrComponents.add("MAPREDUCE2_CLIENT");
>     mrComponents.add("HISTORY_SERVER");
>     serviceComponents.put("MAPREDUCE2", mrComponents);
> 
>     Collection<String> zkComponents = new HashSet<String>();
>     zkComponents.add("ZOOKEEPER_SERVER");
>     zkComponents.add("ZOOKEEPER_CLIENT");
>     serviceComponents.put("ZOOKEEPER", zkComponents);
> 
>     Collection<String> hiveComponents = new HashSet<String>();
>     hiveComponents.add("MYSQL_SERVER");
>     hiveComponents.add("HIVE_METASTORE");
>     serviceComponents.put("HIVE", hiveComponents);
> 
>     Collection<String> falconComponents = new HashSet<String>();
>     falconComponents.add("FALCON_SERVER");
>     falconComponents.add("FALCON_CLIENT");
>     serviceComponents.put("FALCON", falconComponents);
> 
>     Collection<String> gangliaComponents = new HashSet<String>();
>     gangliaComponents.add("GANGLIA_SERVER");
>     gangliaComponents.add("GANGLIA_CLIENT");
>     serviceComponents.put("GANGLIA", gangliaComponents);
> 
>     Collection<String> kafkaComponents = new HashSet<String>();
>     kafkaComponents.add("KAFKA_BROKER");
>     serviceComponents.put("KAFKA", kafkaComponents);
> 
>     Collection<String> knoxComponents = new HashSet<String>();
>     knoxComponents.add("KNOX_GATEWAY");
>     serviceComponents.put("KNOX", knoxComponents);
> 
>     Collection<String> oozieComponents = new HashSet<String>();
>     oozieComponents.add("OOZIE_SERVER");
>     oozieComponents.add("OOZIE_CLIENT");
>     serviceComponents.put("OOZIE", oozieComponents);
> 
>     for (Map.Entry<String, Collection<String>> entry : serviceComponents.entrySet()) {
>       String service = entry.getKey();
>       for (String component : entry.getValue()) {
>         expect(stack.getServiceForComponent(component)).andReturn(service).anyTimes();
>       }
>     }
>   }
> 
>   @After
>   public void tearDown() {
>     reset(bp, serviceInfo, stack);
>   }
> 
49c150
<   public void testDoUpdateForBlueprintExport_SingleHostProperty() {
---
>   public void testDoUpdateForBlueprintExport_SingleHostProperty() throws Exception {
54a156,158
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
59c163
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
64c168
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
66c170
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
70,72c174,178
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     String updatedVal = updatedProperties.get("yarn-site").get("yarn.resourcemanager.hostname");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     String updatedVal = properties.get("yarn-site").get("yarn.resourcemanager.hostname");
75c181
<   
---
> 
77c183
<   public void testDoUpdateForBlueprintExport_SingleHostProperty__withPort() {
---
>   public void testDoUpdateForBlueprintExport_SingleHostProperty__withPort() throws Exception {
82a189,191
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
86c195
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
91c200
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
93c202
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
97,99c206,210
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     String updatedVal = updatedProperties.get("core-site").get("fs.defaultFS");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     String updatedVal = properties.get("core-site").get("fs.defaultFS");
104c215
<   public void testDoUpdateForBlueprintExport_SingleHostProperty__ExternalReference() {
---
>   public void testDoUpdateForBlueprintExport_SingleHostProperty__ExternalReference() throws Exception {
109a221,223
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
114c228
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
119c233
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
121c235
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
125,127c239,243
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     assertFalse(updatedProperties.get("yarn-site").containsKey("yarn.resourcemanager.hostname"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     assertFalse(properties.get("yarn-site").containsKey("yarn.resourcemanager.hostname"));
131c247
<   public void testDoUpdateForBlueprintExport_MultiHostProperty() {
---
>   public void testDoUpdateForBlueprintExport_MultiHostProperty() throws Exception {
136a253,255
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
141c260
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
151c270
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
159c278
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
---
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
161c280
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
166,168c285,289
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     String updatedVal = updatedProperties.get("hbase-site").get("hbase.zookeeper.quorum");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     String updatedVal = properties.get("hbase-site").get("hbase.zookeeper.quorum");
173c294
<   public void testDoUpdateForBlueprintExport_MultiHostProperty__WithPorts() {
---
>   public void testDoUpdateForBlueprintExport_MultiHostProperty__WithPorts() throws Exception {
178a300,302
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
183c307
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
193c317
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
201c325
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
---
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
203c327
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
208,210c332,336
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     String updatedVal = updatedProperties.get("webhcat-site").get("templeton.zookeeper.hosts");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     String updatedVal = properties.get("webhcat-site").get("templeton.zookeeper.hosts");
215c341
<   public void testDoUpdateForBlueprintExport_MultiHostProperty__YAML() {
---
>   public void testDoUpdateForBlueprintExport_MultiHostProperty__YAML() throws Exception {
220a347,349
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
225c354
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
235c364
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
243c372
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
---
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
245c374
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
250,252c379,383
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     String updatedVal = updatedProperties.get("storm-site").get("storm.zookeeper.servers");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     String updatedVal = properties.get("storm-site").get("storm.zookeeper.servers");
257c388
<   public void testDoUpdateForBlueprintExport_DBHostProperty() {
---
>   public void testDoUpdateForBlueprintExport_DBHostProperty() throws Exception {
262a394,396
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
268c402
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
273c407
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
275c409
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
279,281c413,417
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     String updatedVal = updatedProperties.get("hive-site").get("javax.jdo.option.ConnectionURL");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     String updatedVal = properties.get("hive-site").get("javax.jdo.option.ConnectionURL");
286c422
<   public void testDoUpdateForBlueprintExport_DBHostProperty__External() {
---
>   public void testDoUpdateForBlueprintExport_DBHostProperty__External() throws Exception {
291a428,430
>     Configuration clusterConfig = new Configuration(properties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
296c435
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
301c440
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
303c442
<     Collection<HostGroup> hostGroups = new HashSet<HostGroup>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
307,309c446,450
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForBlueprintExport(hostGroups);
<     assertFalse(updatedProperties.get("hive-site").containsKey("javax.jdo.option.ConnectionURL"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
> 
>     assertFalse(properties.get("hive-site").containsKey("javax.jdo.option.ConnectionURL"));
313,317c454,457
<   public void testDoUpdateForClusterCreate_SingleHostProperty__defaultValue() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.resourcemanager.hostname", "localhost");
<     properties.put("yarn-site", typeProps);
---
>   public void testFalconConfigExport() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedPortNum = "808080";
>     final String expectedHostGroupName = "host_group_1";
319,323c459,461
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> falconStartupProperties = new HashMap<String, String>();
>     configProperties.put("falcon-startup.properties", falconStartupProperties);
325,328c463,466
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     
>     falconStartupProperties.put("*.broker.url", expectedHostName + ":" + expectedPortNum);
>     falconStartupProperties.put("*.falcon.service.authentication.kerberos.principal", "falcon/" + expectedHostName + "@EXAMPLE.COM");
>     falconStartupProperties.put("*.falcon.http.authentication.kerberos.principal", "HTTP/" + expectedHostName + "@EXAMPLE.COM");
330,332c468,469
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
334,338c471,479
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("yarn-site").get("yarn.resourcemanager.hostname");
<     assertEquals("testhost", updatedVal);
<   }
---
>     
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("FALCON_SERVER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
340,342c481,482
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__MissingComponent() throws Exception {
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
343a484,486
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
345,346c488,489
<     AmbariManagementController mockMgmtController =
<       mockSupport.createMock(AmbariManagementController.class);
---
>     assertEquals("Falcon Broker URL property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), falconStartupProperties.get("*.broker.url"));
348c491,492
<     expect(mockMgmtController.getStackServices(isA(Set.class))).andReturn(Collections.<StackServiceResponse>emptySet());
---
>     assertEquals("Falcon Kerberos Principal property not properly exported",
>         "falcon/" + "%HOSTGROUP::" + expectedHostGroupName + "%" + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.service.authentication.kerberos.principal"));
350,354c494,496
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.resourcemanager.hostname", "localhost");
<     typeProps.put("yarn.timeline-service.address", "localhost");
<     properties.put("yarn-site", typeProps);
---
>     assertEquals("Falcon Kerberos HTTP Principal property not properly exported",
>         "HTTP/" + "%HOSTGROUP::" + expectedHostGroupName + "%" + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.http.authentication.kerberos.principal"));
>   }
356,360c498,505
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>   @Test
>   public void testDoNameNodeHighAvailabilityExportWithHAEnabled() throws Exception {
>     final String expectedNameService = "mynameservice";
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedPortNum = "808080";
>     final String expectedNodeOne = "nn1";
>     final String expectedNodeTwo = "nn2";
>     final String expectedHostGroupName = "host_group_1";
362,365c507,510
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
>     Map<String, String> hbaseSiteProperties = new HashMap<String, String>();
367,369c512,514
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
---
>     configProperties.put("hdfs-site", hdfsSiteProperties);
>     configProperties.put("core-site", coreSiteProperties);
>     configProperties.put("hbase-site", hbaseSiteProperties);
371c516,518
<     mockSupport.replayAll();
---
>     
>     hdfsSiteProperties.put("dfs.nameservices", expectedNameService);
>     hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice", expectedNodeOne + ", " + expectedNodeTwo);
373,379c520,526
<     Stack testStackDefinition = new Stack("HDP", "2.1", mockMgmtController) {
<       @Override
<       public Cardinality getCardinality(String component) {
<         
<         if (component.equals("APP_TIMELINE_SERVER")) {
<           return new Cardinality("1");
<         }
---
>     
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne, expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo, expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne, expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo, expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne, expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo, expectedHostName + ":" + expectedPortNum);
381,383c528,539
<         return null;
<       }
<     };
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
>     
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("NAMENODE");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
385c541,542
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
387,392c544,546
<     try {
<       updater.doUpdateForClusterCreate(hostGroups, testStackDefinition);
<       fail("IllegalArgumentException should have been thrown");
<     } catch (IllegalArgumentException illegalArgumentException) {
<       
<     }
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
394c548,561
<     mockSupport.verifyAll();
---
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
> 
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
> 
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
398,399c565,567
<   public void testDoUpdateForClusterCreate_SingleHostProperty__MultipleMatchingHostGroupsError() throws Exception {
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>   public void testDoNameNodeHighAvailabilityExportWithHAEnabledNameServicePropertiesIncluded() throws Exception {
>     final String expectedNameService = "mynameservice";
>     final String expectedHostName = "c6401.apache.ambari.org";
400a569,572
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
>     Map<String, String> hbaseSiteProperties = new HashMap<String, String>();
>     Map<String, String> accumuloSiteProperties = new HashMap<String, String>();
402,403c574,576
<     AmbariManagementController mockMgmtController =
<       mockSupport.createMock(AmbariManagementController.class);
---
>     configProperties.put("core-site", coreSiteProperties);
>     configProperties.put("hbase-site", hbaseSiteProperties);
>     configProperties.put("accumulo-site", accumuloSiteProperties);
405c578,583
<     expect(mockMgmtController.getStackServices(isA(Set.class))).andReturn(Collections.<StackServiceResponse>emptySet());
---
>     
>     coreSiteProperties.put("fs.defaultFS", "hdfs:
>     
>     hbaseSiteProperties.put("hbase.rootdir", "hdfs:
>     
>     accumuloSiteProperties.put("instance.volumes", "hdfs:
407,411c585,586
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.resourcemanager.hostname", "localhost");
<     typeProps.put("yarn.timeline-service.address", "localhost");
<     properties.put("yarn-site", typeProps);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
413,418c588,596
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     hgComponents.add("APP_TIMELINE_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("RESOURCEMANAGER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup("group1", groupComponents, hosts);
420,424c598,599
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("APP_TIMELINE_SERVER");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
426,428c601,603
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
430c605,612
<     mockSupport.replayAll();
---
>     
>     assertEquals("Property containing an HA nameservice (fs.defaultFS), was not correctly exported by the processor",
>         "hdfs:
>     assertEquals("Property containing an HA nameservice (hbase.rootdir), was not correctly exported by the processor",
>         "hdfs:
>     assertEquals("Property containing an HA nameservice (instance.volumes), was not correctly exported by the processor",
>         "hdfs:
>   }
432,438c614,619
<     Stack testStackDefinition = new Stack("HDP", "2.1", mockMgmtController) {
<       @Override
<       public Cardinality getCardinality(String component) {
<         
<         if (component.equals("APP_TIMELINE_SERVER")) {
<           return new Cardinality("0-1");
<         }
---
>   @Test
>   public void testDoNameNodeHighAvailabilityExportWithHANotEnabled() throws Exception {
>     
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     configProperties.put("hdfs-site", hdfsSiteProperties);
440,442c621,622
<         return null;
<       }
<     };
---
>     assertEquals("Incorrect initial state for hdfs-site config",
>         0, hdfsSiteProperties.size());
444c624,625
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
446,451c627,632
<     try {
<       updater.doUpdateForClusterCreate(hostGroups, testStackDefinition);
<       fail("IllegalArgumentException should have been thrown");
<     } catch (IllegalArgumentException illegalArgumentException) {
<       
<     }
---
>     
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("NAMENODE");
>     TestHostGroup group = new TestHostGroup("group1", groupComponents, Collections.singleton("host1"));
453,454c634,635
<     mockSupport.verifyAll();
<   }
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
456,458c637,639
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__MissingOptionalComponent() throws Exception {
<     final String expectedHostName = "localhost";
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
460c641,643
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     assertEquals("Incorrect state for hdfs-site config after HA call in non-HA environment, should be zero",
>         0, hdfsSiteProperties.size());
>   }
462,463c645,650
<     AmbariManagementController mockMgmtController =
<       mockSupport.createMock(AmbariManagementController.class);
---
>   @Test
>   public void testDoNameNodeHighAvailabilityExportWithHAEnabledMultipleServices() throws Exception {
>     final String expectedNameServiceOne = "mynameserviceOne";
>     final String expectedNameServiceTwo = "mynameserviceTwo";
>     final String expectedHostNameOne = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.apache.ambari.org";
465c652,655
<     expect(mockMgmtController.getStackServices(isA(Set.class))).andReturn(Collections.<StackServiceResponse>emptySet());
---
>     final String expectedPortNum = "808080";
>     final String expectedNodeOne = "nn1";
>     final String expectedNodeTwo = "nn2";
>     final String expectedHostGroupName = "host_group_1";
467,470c657,659
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.timeline-service.address", expectedHostName);
<     properties.put("yarn-site", typeProps);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     configProperties.put("hdfs-site", hdfsSiteProperties);
472,476c661,664
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
---
>     
>     hdfsSiteProperties.put("dfs.nameservices", expectedNameServiceOne + "," + expectedNameServiceTwo);
>     hdfsSiteProperties.put("dfs.ha.namenodes." + expectedNameServiceOne, expectedNodeOne + ", " + expectedNodeTwo);
>     hdfsSiteProperties.put("dfs.ha.namenodes." + expectedNameServiceTwo, expectedNodeOne + ", " + expectedNodeTwo);
478,481c666,672
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeOne, expectedHostNameOne + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeTwo, expectedHostNameOne + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeOne, expectedHostNameOne + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeTwo, expectedHostNameOne + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeOne, expectedHostNameOne + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeTwo, expectedHostNameOne + ":" + expectedPortNum);
483,485c674,680
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
---
>     
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeOne, expectedHostNameTwo + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeTwo, expectedHostNameTwo + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeOne, expectedHostNameTwo + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeTwo, expectedHostNameTwo + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeOne, expectedHostNameTwo + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeTwo, expectedHostNameTwo + ":" + expectedPortNum);
487c682,683
<     mockSupport.replayAll();
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
489,495c685,694
<     Stack testStackDefinition = new Stack("HDP", "2.1", mockMgmtController) {
<       @Override
<       public Cardinality getCardinality(String component) {
<         
<         if (component.equals("APP_TIMELINE_SERVER")) {
<           return new Cardinality("0-1");
<         }
---
>     
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("RESOURCEMANAGER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostNameOne);
>     hosts.add(expectedHostNameTwo);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
497,499c696,697
<         return null;
<       }
<     };
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
501c699,701
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
503,506c703,707
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, testStackDefinition);
<     String updatedVal = updatedProperties.get("yarn-site").get("yarn.timeline-service.address");
<     assertEquals("Timeline Server config property should not have been updated",
<       expectedHostName, updatedVal);
---
>     
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeTwo));
508,509c709,712
<     mockSupport.verifyAll();
<   }
---
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeTwo));
511,516c714,717
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__defaultValue__WithPort() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("fs.defaultFS", "localhost:5050");
<     properties.put("core-site", typeProps);
---
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeTwo));
518,522d718
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
524,527c720,724
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
---
>     
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeTwo));
529,531c726,729
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
---
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeTwo));
533,536c731,734
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("core-site").get("fs.defaultFS");
<     assertEquals("testhost:5050", updatedVal);
---
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeTwo));
540,544c738,741
<   public void testDoUpdateForClusterCreate_MultiHostProperty__defaultValues() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("hbase.zookeeper.quorum", "localhost");
<     properties.put("hbase-site", typeProps);
---
>   public void testYarnConfigExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedPortNum = "808080";
>     final String expectedHostGroupName = "host_group_1";
546,1234c743,745
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hbase-site").get("hbase.zookeeper.quorum");
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("testhost");
<     expectedHosts.add("testhost2");
<     expectedHosts.add("testhost2a");
<     expectedHosts.add("testhost2b");
< 
<     assertEquals(4, hosts.length);
<     for (String host : hosts) {
<       assertTrue(expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty__defaultValues___withPorts() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("templeton.zookeeper.hosts", "localhost:9090");
<     properties.put("webhcat-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("webhcat-site").get("templeton.zookeeper.hosts");
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("testhost:9090");
<     expectedHosts.add("testhost2:9090");
<     expectedHosts.add("testhost2a:9090");
<     expectedHosts.add("testhost2b:9090");
< 
<     assertEquals(4, hosts.length);
<     for (String host : hosts) {
<       assertTrue(expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty__defaultValues___YAML() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("storm.zookeeper.servers", "['localhost']");
<     properties.put("storm-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("storm-site").get("storm.zookeeper.servers");
<     assertTrue(updatedVal.startsWith("["));
<     assertTrue(updatedVal.endsWith("]"));
<     
<     updatedVal = updatedVal.replaceAll("[\\[\\]]", "");
< 
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("'testhost'");
<     expectedHosts.add("'testhost2'");
<     expectedHosts.add("'testhost2a'");
<     expectedHosts.add("'testhost2b'");
< 
<     assertEquals(4, hosts.length);
<     for (String host : hosts) {
<       assertTrue(expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MProperty__defaultValues() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("hbase_master_heapsize", "512m");
<     properties.put("hbase-env", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hbase-env").get("hbase_master_heapsize");
<     assertEquals("512m", updatedVal);
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MProperty__missingM() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("hbase_master_heapsize", "512");
<     properties.put("hbase-env", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hbase-env").get("hbase_master_heapsize");
<     assertEquals("512m", updatedVal);
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.resourcemanager.hostname", "%HOSTGROUP::group1%");
<     properties.put("yarn-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("yarn-site").get("yarn.resourcemanager.hostname");
<     assertEquals("testhost", updatedVal);
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue_UsingMinusSymbolInHostGroupName() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.resourcemanager.hostname", "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%");
<     properties.put("yarn-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-6", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("yarn-site").get("yarn.resourcemanager.hostname");
<     assertEquals("testhost", updatedVal);
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue_WithPort_UsingMinusSymbolInHostGroupName() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("yarn.resourcemanager.hostname", "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%:2180");
<     properties.put("yarn-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-6", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("yarn-site").get("yarn.resourcemanager.hostname");
<     assertEquals("testhost:2180", updatedVal);
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue__WithPort() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("fs.defaultFS", "%HOSTGROUP::group1%:5050");
<     properties.put("core-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("core-site").get("fs.defaultFS");
<     assertEquals("testhost:5050", updatedVal);
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("hbase.zookeeper.quorum", "%HOSTGROUP::group1%,%HOSTGROUP::group2%");
<     properties.put("hbase-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hbase-site").get("hbase.zookeeper.quorum");
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("testhost");
<     expectedHosts.add("testhost2");
<     expectedHosts.add("testhost2a");
<     expectedHosts.add("testhost2b");
< 
<     assertEquals(4, hosts.length);
<     for (String host : hosts) {
<       assertTrue(expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues___withPorts() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("templeton.zookeeper.hosts", "%HOSTGROUP::group1%:9090,%HOSTGROUP::group2%:9091");
<     properties.put("webhcat-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("webhcat-site").get("templeton.zookeeper.hosts");
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("testhost:9090");
<     expectedHosts.add("testhost2:9091");
<     expectedHosts.add("testhost2a:9091");
<     expectedHosts.add("testhost2b:9091");
< 
<     assertEquals(4, hosts.length);
<     for (String host : hosts) {
<       assertTrue(expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues___withPorts_UsingMinusSymbolInHostGroupName() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("ha.zookeeper.quorum", "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%:2181,%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-5%:2181,%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-7%:2181");
<     properties.put("core-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-6", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-5", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-7", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("core-site").get("ha.zookeeper.quorum");
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("testhost:2181");
<     expectedHosts.add("testhost2:2181");
<     expectedHosts.add("testhost2a:2181");
<     expectedHosts.add("testhost2b:2181");
<     expectedHosts.add("testhost3:2181");
<     expectedHosts.add("testhost3a:2181");
< 
<     assertEquals(6, hosts.length);
<     for (String host : hosts) {
<       assertTrue("Expected host :" + host + "was not included in the multi-server list in this property.", expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty_exportedValues_withPorts_singleHostValue() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> yarnSiteConfig = new HashMap<String, String>();
< 
<     yarnSiteConfig.put("hadoop.registry.zk.quorum", "%HOSTGROUP::host_group_1%:2181");
<     properties.put("yarn-site", yarnSiteConfig);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("host_group_1", Collections.singleton("testhost"), hgComponents);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     assertEquals("Multi-host property with single host value was not correctly updated for cluster create.",
<       "testhost:2181", updatedProperties.get("yarn-site").get("hadoop.registry.zk.quorum"));
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues___YAML() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("storm.zookeeper.servers", "['%HOSTGROUP::group1%:9090','%HOSTGROUP::group2%:9091']");
<     properties.put("storm-site", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("ZOOKEEPER_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_SERVER");
<     Set<String> hosts2 = new HashSet<String>();
<     hosts2.add("testhost2");
<     hosts2.add("testhost2a");
<     hosts2.add("testhost2b");
<     HostGroup group2 = new TestHostGroup("group2", hosts2, hgComponents2);
< 
<     Collection<String> hgComponents3 = new HashSet<String>();
<     hgComponents2.add("HDFS_CLIENT");
<     hgComponents2.add("ZOOKEEPER_CLIENT");
<     Set<String> hosts3 = new HashSet<String>();
<     hosts3.add("testhost3");
<     hosts3.add("testhost3a");
<     HostGroup group3 = new TestHostGroup("group3", hosts3, hgComponents3);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
<     hostGroups.put(group3.getName(), group3);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("storm-site").get("storm.zookeeper.servers");
<     assertTrue(updatedVal.startsWith("["));
<     assertTrue(updatedVal.endsWith("]"));
<     
<     updatedVal = updatedVal.replaceAll("[\\[\\]]", "");
< 
<     String[] hosts = updatedVal.split(",");
< 
<     Collection<String> expectedHosts = new HashSet<String>();
<     expectedHosts.add("'testhost:9090'");
<     expectedHosts.add("'testhost2:9091'");
<     expectedHosts.add("'testhost2a:9091'");
<     expectedHosts.add("'testhost2b:9091'");
< 
<     assertEquals(4, hosts.length);
<     for (String host : hosts) {
<       assertTrue(expectedHosts.contains(host));
<       expectedHosts.remove(host);
<     }
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_DBHostProperty__defaultValue() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> hiveSiteProps = new HashMap<String, String>();
<     hiveSiteProps.put("javax.jdo.option.ConnectionURL", "jdbc:mysql:
<     Map<String, String> hiveEnvProps = new HashMap<String, String>();
<     hiveEnvProps.put("hive_database", "New MySQL Database");
<     properties.put("hive-site", hiveSiteProps);
<     properties.put("hive-env", hiveEnvProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     hgComponents.add("MYSQL_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hive-site").get("javax.jdo.option.ConnectionURL");
<     assertEquals("jdbc:mysql:
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_DBHostProperty__exportedValue() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> hiveSiteProps = new HashMap<String, String>();
<     hiveSiteProps.put("javax.jdo.option.ConnectionURL", "jdbc:mysql:
<     Map<String, String> hiveEnvProps = new HashMap<String, String>();
<     hiveEnvProps.put("hive_database", "New MySQL Database");
<     properties.put("hive-site", hiveSiteProps);
<     properties.put("hive-env", hiveEnvProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     hgComponents.add("MYSQL_SERVER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hive-site").get("javax.jdo.option.ConnectionURL");
<     assertEquals("jdbc:mysql:
<   }
< 
<   @Test
<   public void testDoUpdateForClusterCreate_DBHostProperty__external() {
<     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
<     Map<String, String> typeProps = new HashMap<String, String>();
<     typeProps.put("javax.jdo.option.ConnectionURL", "jdbc:mysql:
<     typeProps.put("hive_database", "Existing MySQL Database");
<     properties.put("hive-env", typeProps);
< 
<     Collection<String> hgComponents = new HashSet<String>();
<     hgComponents.add("NAMENODE");
<     hgComponents.add("SECONDARY_NAMENODE");
<     hgComponents.add("RESOURCEMANAGER");
<     HostGroup group1 = new TestHostGroup("group1", Collections.singleton("testhost"), hgComponents);
< 
<     Collection<String> hgComponents2 = new HashSet<String>();
<     hgComponents2.add("DATANODE");
<     hgComponents2.add("HDFS_CLIENT");
<     HostGroup group2 = new TestHostGroup("group2", Collections.singleton("testhost2"), hgComponents2);
< 
<     Map<String, HostGroup> hostGroups = new HashMap<String, HostGroup>();
<     hostGroups.put(group1.getName(), group1);
<     hostGroups.put(group2.getName(), group2);
< 
<     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(properties);
<     Map<String, Map<String, String>> updatedProperties = updater.doUpdateForClusterCreate(hostGroups, null);
<     String updatedVal = updatedProperties.get("hive-env").get("javax.jdo.option.ConnectionURL");
<     assertEquals("jdbc:mysql:
<   }
< 
<   @Test
<   public void testFalconConfigExport() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedHostGroupName = "host_group_1";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
< 
<     mockSupport.replayAll();
< 
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> falconStartupProperties =
<       new HashMap<String, String>();
< 
<     configProperties.put("falcon-startup.properties", falconStartupProperties);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> yarnSiteProperties = new HashMap<String, String>();
>     configProperties.put("yarn-site", yarnSiteProperties);
1237,1239c748,757
<     falconStartupProperties.put("*.broker.url", expectedHostName + ":" + expectedPortNum);
<     falconStartupProperties.put("*.falcon.service.authentication.kerberos.principal", "falcon/" + expectedHostName + "@EXAMPLE.COM");
<     falconStartupProperties.put("*.falcon.http.authentication.kerberos.principal", "HTTP/" + expectedHostName + "@EXAMPLE.COM");
---
>     yarnSiteProperties.put("yarn.log.server.url", "http:
>     yarnSiteProperties.put("yarn.resourcemanager.hostname", expectedHostName);
>     yarnSiteProperties.put("yarn.resourcemanager.resource-tracker.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.webapp.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.scheduler.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.admin.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.webapp.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.webapp.https.address", expectedHostName + ":" + expectedPortNum);
1241,1242c759,760
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1245,1251c763,770
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
< 
<     assertEquals("Falcon Broker URL property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), falconStartupProperties.get("*.broker.url"));
< 
<     assertEquals("Falcon Kerberos Principal property not properly exported",
<       "falcon/" + "%HOSTGROUP::" + expectedHostGroupName + "%" + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.service.authentication.kerberos.principal"));
---
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("RESOURCEMANAGER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
1253,1254c772,773
<     assertEquals("Falcon Kerberos HTTP Principal property not properly exported",
<       "HTTP/" + "%HOSTGROUP::" + expectedHostGroupName + "%" + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.http.authentication.kerberos.principal"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
1256c775,777
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
1257a779,798
>     assertEquals("Yarn Log Server URL was incorrectly exported",
>         "http:
>     assertEquals("Yarn ResourceManager hostname was incorrectly exported",
>         createExportedHostName(expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.hostname"));
>     assertEquals("Yarn ResourceManager tracker address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.resource-tracker.address"));
>     assertEquals("Yarn ResourceManager webapp address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.webapp.address"));
>     assertEquals("Yarn ResourceManager scheduler address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.scheduler.address"));
>     assertEquals("Yarn ResourceManager address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.address"));
>     assertEquals("Yarn ResourceManager admin address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.admin.address"));
>     assertEquals("Yarn ResourceManager timeline-service address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.timeline-service.address"));
>     assertEquals("Yarn ResourceManager timeline webapp address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.timeline-service.webapp.address"));
>     assertEquals("Yarn ResourceManager timeline webapp HTTPS address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.timeline-service.webapp.https.address"));
1261c802
<   public void testFalconConfigClusterUpdate() throws Exception {
---
>   public void testYarnConfigExportedWithDefaultZeroHostAddress() throws Exception {
1266,1280c807,809
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
< 
<     mockSupport.replayAll();
< 
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> falconStartupProperties =
<       new HashMap<String, String>();
< 
<     configProperties.put("falcon-startup.properties", falconStartupProperties);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> yarnSiteProperties = new HashMap<String, String>();
>     configProperties.put("yarn-site", yarnSiteProperties);
1283,1288c812,821
<     falconStartupProperties.put("*.broker.url", createExportedAddress(expectedPortNum, expectedHostGroupName));
<     falconStartupProperties.put("*.falcon.service.authentication.kerberos.principal", "falcon/" + createExportedHostName(expectedHostGroupName) + "@EXAMPLE.COM");
<     falconStartupProperties.put("*.falcon.http.authentication.kerberos.principal", "HTTP/" + createExportedHostName(expectedHostGroupName) + "@EXAMPLE.COM");
< 
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     yarnSiteProperties.put("yarn.log.server.url", "http:
>     yarnSiteProperties.put("yarn.resourcemanager.hostname", expectedHostName);
>     yarnSiteProperties.put("yarn.resourcemanager.resource-tracker.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.webapp.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.scheduler.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.admin.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.address", "0.0.0.0" + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.webapp.address", "0.0.0.0" + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.webapp.https.address", "0.0.0.0" + ":" + expectedPortNum);
1290,1292c823,824
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1295,1301c827,834
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
< 
<     assertEquals("Falcon Broker URL property not properly exported",
<       expectedHostName + ":" + expectedPortNum, falconStartupProperties.get("*.broker.url"));
< 
<     assertEquals("Falcon Kerberos Principal property not properly exported",
<       "falcon/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.service.authentication.kerberos.principal"));
---
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("RESOURCEMANAGER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
1303,1304c836,837
<     assertEquals("Falcon Kerberos HTTP Principal property not properly exported",
<       "HTTP/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.http.authentication.kerberos.principal"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
1306c839,841
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
1307a843,862
>     assertEquals("Yarn Log Server URL was incorrectly exported",
>         "http:
>     assertEquals("Yarn ResourceManager hostname was incorrectly exported",
>         createExportedHostName(expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.hostname"));
>     assertEquals("Yarn ResourceManager tracker address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.resource-tracker.address"));
>     assertEquals("Yarn ResourceManager webapp address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.webapp.address"));
>     assertEquals("Yarn ResourceManager scheduler address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.scheduler.address"));
>     assertEquals("Yarn ResourceManager address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.address"));
>     assertEquals("Yarn ResourceManager admin address was incorrectly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.admin.address"));
>     assertEquals("Yarn ResourceManager timeline-service address was incorrectly exported",
>         "0.0.0.0" + ":" + expectedPortNum, yarnSiteProperties.get("yarn.timeline-service.address"));
>     assertEquals("Yarn ResourceManager timeline webapp address was incorrectly exported",
>         "0.0.0.0" + ":" + expectedPortNum, yarnSiteProperties.get("yarn.timeline-service.webapp.address"));
>     assertEquals("Yarn ResourceManager timeline webapp HTTPS address was incorrectly exported",
>         "0.0.0.0" + ":" + expectedPortNum, yarnSiteProperties.get("yarn.timeline-service.webapp.https.address"));
1311c866
<   public void testFalconConfigClusterUpdateDefaultConfig() throws Exception {
---
>   public void testHDFSConfigExported() throws Exception {
1316,1338c871,875
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     expect(mockHostGroupOne.getComponents()).andReturn(Arrays.asList("FALCON_SERVER")).atLeastOnce();
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
< 
<     mockSupport.replayAll();
< 
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> falconStartupProperties =
<       new HashMap<String, String>();
< 
<     configProperties.put("falcon-startup.properties", falconStartupProperties);
< 
<     
<     falconStartupProperties.put("*.broker.url", "localhost:" + expectedPortNum);
<     falconStartupProperties.put("*.falcon.service.authentication.kerberos.principal", "falcon/" + "localhost" + "@EXAMPLE.COM");
<     falconStartupProperties.put("*.falcon.http.authentication.kerberos.principal", "HTTP/" + "localhost" + "@EXAMPLE.COM");
< 
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
>     Map<String, String> hbaseSiteProperties = new HashMap<String, String>();
>     Map<String, String> accumuloSiteProperties = new HashMap<String, String>();
1340,1342c877,880
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
---
>     configProperties.put("hdfs-site", hdfsSiteProperties);
>     configProperties.put("core-site", coreSiteProperties);
>     configProperties.put("hbase-site", hbaseSiteProperties);
>     configProperties.put("accumulo-site", accumuloSiteProperties);
1345,1369c883,889
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
< 
<     assertEquals("Falcon Broker URL property not properly exported",
<       expectedHostName + ":" + expectedPortNum, falconStartupProperties.get("*.broker.url"));
< 
<     assertEquals("Falcon Kerberos Principal property not properly exported",
<       "falcon/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.service.authentication.kerberos.principal"));
< 
<     assertEquals("Falcon Kerberos HTTP Principal property not properly exported",
<       "HTTP/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.http.authentication.kerberos.principal"));
< 
<     mockSupport.verifyAll();
< 
<   }
< 
<   @Test
<   public void testHiveConfigClusterUpdateCustomValue() throws Exception {
<     final String expectedHostGroupName = "host_group_1";
< 
<     final String expectedPropertyValue =
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     hdfsSiteProperties.put("dfs.http.address", expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.https.address", expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.http-address", expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.https-address", expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.secondary.http.address", expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.secondary.http-address", expectedHostName + ":" + expectedPortNum);
>     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", expectedHostName + ":" + expectedPortNum);
1371c891,892
<     mockSupport.replayAll();
---
>     coreSiteProperties.put("fs.default.name", expectedHostName + ":" + expectedPortNum);
>     coreSiteProperties.put("fs.defaultFS", "hdfs:
1373,1374c894
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     hbaseSiteProperties.put("hbase.rootdir", "hdfs:
1376,1377c896
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
---
>     accumuloSiteProperties.put("instance.volumes", "hdfs:
1379c898,899
<     configProperties.put("webhcat-site", webHCatSiteProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1382,1391d901
<     webHCatSiteProperties.put("templeton.hive.properties",
<       expectedPropertyValue);
< 
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
< 
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
< 
1393,1427d902
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
< 
<     assertEquals("Unexpected config update for templeton.hive.properties",
<       expectedPropertyValue,
<       webHCatSiteProperties.get("templeton.hive.properties"));
< 
< 
<     mockSupport.verifyAll();
< 
<   }
< 
<   @Test
<   public void testHiveConfigClusterUpdateCustomValueSpecifyingHostNamesMetaStoreHA() throws Exception {
<     final String expectedHostGroupName = "host_group_1";
< 
<     final String expectedPropertyValue =
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     Stack mockStack = mockSupport.createMock(Stack.class);
< 
<     mockSupport.replayAll();
< 
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
< 
<     configProperties.put("webhcat-site", webHCatSiteProperties);
< 
1429,1430c904,910
<     webHCatSiteProperties.put("templeton.hive.properties",
<       expectedPropertyValue);
---
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("NAMENODE");
>     groupComponents.add("SECONDARY_NAMENODE");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
1432,1433c912,913
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
1435,1438c915,917
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put("host_group_2", mockHostGroupTwo);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
>     configProcessor.doUpdateForBlueprintExport();
1440,1441c919,932
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.http.address"));
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.https.address"));
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address"));
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address"));
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.secondary.http.address"));
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.secondary.http-address"));
>     assertEquals("hdfs config property not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
1443,1445c934,937
<     assertEquals("Unexpected config update for templeton.hive.properties",
<       expectedPropertyValue,
<       webHCatSiteProperties.get("templeton.hive.properties"));
---
>     assertEquals("hdfs config in core-site not exported properly",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), coreSiteProperties.get("fs.default.name"));
>     assertEquals("hdfs config in core-site not exported properly",
>         "hdfs:
1447c939,940
<     mockSupport.verifyAll();
---
>     assertEquals("hdfs config in hbase-site not exported properly",
>         "hdfs:
1448a942,943
>     assertEquals("hdfs config in accumulo-site not exported properly",
>         "hdfs:
1452c947,950
<   public void testHiveConfigClusterUpdateSpecifyingHostNamesHiveServer2HA() throws Exception {
---
>   public void testHiveConfigExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.ambari.apache.org";
>     final String expectedPortNum = "808080";
1453a952
>     final String expectedHostGroupNameTwo = "host_group_2";
1455,1481c954,958
<     final String expectedPropertyValue =
<       "c6401.ambari.apache.org";
< 
<     final String expectedMetaStoreURIs = "thrift:
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     Stack mockStack = mockSupport.createMock(Stack.class);
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("HIVE_SERVER")).atLeastOnce();
<     expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("HIVE_SERVER")).atLeastOnce();
< 
<     
<     expect(mockStack.getCardinality("HIVE_SERVER")).andReturn(new Cardinality("1+")).atLeastOnce();
< 
<     mockSupport.replayAll();
< 
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> hiveEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> hiveSiteProperties =
<       new HashMap<String, String>();
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hiveSiteProperties = new HashMap<String, String>();
>     Map<String, String> hiveEnvProperties = new HashMap<String, String>();
>     Map<String, String> webHCatSiteProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
1483d959
<     configProperties.put("hive-env", hiveEnvProperties);
1484a961,963
>     configProperties.put("hive-env", hiveEnvProperties);
>     configProperties.put("webhcat-site", webHCatSiteProperties);
>     configProperties.put("core-site", coreSiteProperties);
1487,1523c966,970
<     hiveEnvProperties.put("hive_hostname",
<       expectedPropertyValue);
< 
<     
<     hiveSiteProperties.put("hive.server2.support.dynamic.service.discovery", "true");
< 
<     
< 
<     hiveSiteProperties.put("hive.metastore.uris", expectedMetaStoreURIs);
< 
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
< 
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put("host_group_2", mockHostGroupTwo);
< 
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
< 
<     assertEquals("Unexpected config update for hive_hostname",
<       expectedPropertyValue,
<       hiveEnvProperties.get("hive_hostname"));
< 
<     assertEquals("Unexpected config update for hive.metastore.uris",
<       expectedMetaStoreURIs,
<       hiveSiteProperties.get("hive.metastore.uris"));
< 
<       mockSupport.verifyAll();
< 
<   }
< 
<   @Test
<   public void testHiveConfigClusterUpdateUsingExportedNamesHiveServer2HA() throws Exception {
<     final String expectedHostGroupNameOne = "host_group_1";
<     final String expectedHostGroupNameTwo = "host_group_2";
---
>     hiveSiteProperties.put("hive.metastore.uris", "thrift:
>     hiveSiteProperties.put("javax.jdo.option.ConnectionURL", expectedHostName + ":" + expectedPortNum);
>     hiveSiteProperties.put("hive.zookeeper.quorum", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
>     hiveSiteProperties.put("hive.cluster.delegation.token.store.zookeeper.connectString", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
>     hiveEnvProperties.put("hive_hostname", expectedHostName);
1525,1526c972,973
<     final String expectedHostNameOne =
<       "c6401.ambari.apache.org";
---
>     webHCatSiteProperties.put("templeton.hive.properties", expectedHostName + "," + expectedHostNameTwo);
>     webHCatSiteProperties.put("templeton.kerberos.principal", expectedHostName);
1528,1529c975,977
<     final String expectedHostNameTwo =
<       "c6402.ambari.apache.org";
---
>     coreSiteProperties.put("hadoop.proxyuser.hive.hosts", expectedHostName + "," + expectedHostNameTwo);
>     coreSiteProperties.put("hadoop.proxyuser.HTTP.hosts", expectedHostName + "," + expectedHostNameTwo);
>     coreSiteProperties.put("hadoop.proxyuser.hcat.hosts", expectedHostName + "," + expectedHostNameTwo);
1530a979,980
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1534c984,990
<     final String inputMetaStoreURIs = "thrift:
---
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("HIVE_SERVER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
1536c992,1001
<     final String expectedMetaStoreURIs = "thrift:
---
>     Collection<String> groupComponents2 = new HashSet<String>();
>     groupComponents2.add("HIVE_CLIENT");
>     Collection<String> hosts2 = new ArrayList<String>();
>     hosts2.add(expectedHostNameTwo);
>     hosts2.add("serverFour");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, groupComponents2, hosts2);
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
>     hostGroups.add(group2);
1538c1003,1014
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
> 
>     
>     configProcessor.doUpdateForBlueprintExport();
> 
>     assertEquals("hive property not properly exported",
>         "thrift:
>     assertEquals("hive property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hiveSiteProperties.get("javax.jdo.option.ConnectionURL"));
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName), hiveEnvProperties.get("hive_hostname"));
1540c1016,1020
<     Stack mockStack = mockSupport.createMock(Stack.class);
---
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         webHCatSiteProperties.get("templeton.hive.properties"));
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName), webHCatSiteProperties.get("templeton.kerberos.principal"));
1542,1543c1022,1023
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hive.hosts"));
1545,1546c1025,1026
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Collections.singleton(expectedHostNameOne)).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Collections.singleton(expectedHostNameTwo)).atLeastOnce();
---
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.HTTP.hosts"));
1547a1028,1029
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hcat.hosts"));
1549,1551c1031,1033
<     Set<String> setOfComponents = new HashSet<String>();
<     setOfComponents.add("HIVE_SERVER");
<     setOfComponents.add("HIVE_METASTORE");
---
>     assertEquals("hive zookeeper quorum property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
>         hiveSiteProperties.get("hive.zookeeper.quorum"));
1553,1554c1035,1037
<     expect(mockHostGroupOne.getComponents()).andReturn(setOfComponents).atLeastOnce();
<     expect(mockHostGroupTwo.getComponents()).andReturn(setOfComponents).atLeastOnce();
---
>     assertEquals("hive zookeeper connectString property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
>         hiveSiteProperties.get("hive.cluster.delegation.token.store.zookeeper.connectString"));
1556,1557c1039
<     
<     expect(mockStack.getCardinality("HIVE_SERVER")).andReturn(new Cardinality("1+")).atLeastOnce();
---
>   }
1559c1041,1047
<     mockSupport.replayAll();
---
>   @Test
>   public void testHiveConfigExportedMultipleHiveMetaStoreServers() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.ambari.apache.org";
>     final String expectedPortNum = "808080";
>     final String expectedHostGroupName = "host_group_1";
>     final String expectedHostGroupNameTwo = "host_group_2";
1562c1050
<       new HashMap<String, Map<String, String>>();
---
>         new HashMap<String, Map<String, String>>();
1564,1565d1051
<     Map<String, String> hiveEnvProperties =
<       new HashMap<String, String>();
1567c1053,1059
<       new HashMap<String, String>();
---
>         new HashMap<String, String>();
>     Map<String, String> hiveEnvProperties =
>         new HashMap<String, String>();
>     Map<String, String> webHCatSiteProperties =
>         new HashMap<String, String>();
>     Map<String, String> coreSiteProperties =
>         new HashMap<String, String>();
1569d1060
<     configProperties.put("hive-env", hiveEnvProperties);
1570a1062,1064
>     configProperties.put("hive-env", hiveEnvProperties);
>     configProperties.put("webhcat-site", webHCatSiteProperties);
>     configProperties.put("core-site", coreSiteProperties);
1573,1574c1067,1071
<     hiveEnvProperties.put("hive_hostname",
<       expectedHostNameOne);
---
>     hiveSiteProperties.put("hive.metastore.uris", "thrift:
>     hiveSiteProperties.put("javax.jdo.option.ConnectionURL", expectedHostName + ":" + expectedPortNum);
>     hiveSiteProperties.put("hive.zookeeper.quorum", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
>     hiveSiteProperties.put("hive.cluster.delegation.token.store.zookeeper.connectString", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
>     hiveEnvProperties.put("hive_hostname", expectedHostName);
1576,1577c1073,1074
<     
<     hiveSiteProperties.put("hive.server2.support.dynamic.service.discovery", "true");
---
>     webHCatSiteProperties.put("templeton.hive.properties", expectedHostName + "," + expectedHostNameTwo);
>     webHCatSiteProperties.put("templeton.kerberos.principal", expectedHostName);
1579c1076,1078
<     
---
>     coreSiteProperties.put("hadoop.proxyuser.hive.hosts", expectedHostName + "," + expectedHostNameTwo);
>     coreSiteProperties.put("hadoop.proxyuser.HTTP.hosts", expectedHostName + "," + expectedHostNameTwo);
>     coreSiteProperties.put("hadoop.proxyuser.hcat.hosts", expectedHostName + "," + expectedHostNameTwo);
1581c1080,1081
<     hiveSiteProperties.put("hive.metastore.uris", inputMetaStoreURIs);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1583,1584c1083,1095
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("NAMENODE");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
> 
>     Collection<String> groupComponents2 = new HashSet<String>();
>     groupComponents2.add("DATANODE");
>     Collection<String> hosts2 = new ArrayList<String>();
>     hosts2.add(expectedHostNameTwo);
>     hosts2.add("serverThree");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, groupComponents2, hosts2);
1586,1589c1097,1102
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupNameOne, mockHostGroupOne);
<     mapOfHostGroups.put(expectedHostGroupNameTwo, mockHostGroupTwo);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
>     hostGroups.add(group2);
> 
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
1592c1105
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     configProcessor.doUpdateForBlueprintExport();
1594,1596c1107
<     assertEquals("Unexpected config update for hive_hostname",
<       expectedHostNameOne,
<       hiveEnvProperties.get("hive_hostname"));
---
>     System.out.println("RWN: exported value of hive.metastore.uris = " + hiveSiteProperties.get("hive.metastore.uris"));
1598,1600c1109,1126
<     assertEquals("Unexpected config update for hive.metastore.uris",
<       expectedMetaStoreURIs,
<       hiveSiteProperties.get("hive.metastore.uris"));
---
>     assertEquals("hive property not properly exported",
>         "thrift:
>     assertEquals("hive property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName), hiveSiteProperties.get("javax.jdo.option.ConnectionURL"));
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName), hiveEnvProperties.get("hive_hostname"));
> 
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         webHCatSiteProperties.get("templeton.hive.properties"));
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName), webHCatSiteProperties.get("templeton.kerberos.principal"));
> 
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hive.hosts"));
> 
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.HTTP.hosts"));
1602c1128,1133
<     mockSupport.verifyAll();
---
>     assertEquals("hive property not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hcat.hosts"));
> 
>     assertEquals("hive zookeeper quorum property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
>         hiveSiteProperties.get("hive.zookeeper.quorum"));
1603a1135,1137
>     assertEquals("hive zookeeper connectString property not properly exported",
>         createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
>         hiveSiteProperties.get("hive.cluster.delegation.token.store.zookeeper.connectString"));
1607c1141,1144
<   public void testHiveConfigClusterUpdateDefaultValue() throws Exception {
---
>   public void testOozieConfigExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.ambari.apache.org";
>     final String expectedExternalHost = "c6408.ambari.apache.org";
1609,1612c1146
<     final String expectedHostName = "c6401.ambari.apache.org";
< 
<     final String expectedPropertyValue =
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
---
>     final String expectedHostGroupNameTwo = "host_group_2";
1614c1148,1151
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> oozieSiteProperties = new HashMap<String, String>();
>     Map<String, String> oozieEnvProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
1616,1618c1153,1156
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("HIVE_METASTORE")).atLeastOnce();
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Collections.singleton(expectedHostName)).atLeastOnce();
---
>     configProperties.put("oozie-site", oozieSiteProperties);
>     configProperties.put("oozie-env", oozieEnvProperties);
>     configProperties.put("hive-env", oozieEnvProperties);
>     configProperties.put("core-site", coreSiteProperties);
1620c1158,1161
<     mockSupport.replayAll();
---
>     oozieSiteProperties.put("oozie.base.url", expectedHostName);
>     oozieSiteProperties.put("oozie.authentication.kerberos.principal", expectedHostName);
>     oozieSiteProperties.put("oozie.service.HadoopAccessorService.kerberos.principal", expectedHostName);
>     oozieSiteProperties.put("oozie.service.JPAService.jdbc.url", "jdbc:mysql:
1622,1623c1163,1164
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     oozieEnvProperties.put("oozie_hostname", expectedHostName);
>     oozieEnvProperties.put("oozie_existing_mysql_host", expectedExternalHost);
1625,1626c1166
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
---
>     coreSiteProperties.put("hadoop.proxyuser.oozie.hosts", expectedHostName + "," + expectedHostNameTwo);
1628c1168,1169
<     configProperties.put("webhcat-site", webHCatSiteProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1631,1632c1172,1179
<     webHCatSiteProperties.put("templeton.hive.properties",
<       expectedPropertyValue);
---
>     
>     
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("OOZIE_SERVER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
1634,1635c1181,1186
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> groupComponents2 = new HashSet<String>();
>     groupComponents2.add("OOZIE_SERVER");
>     Collection<String> hosts2 = new ArrayList<String>();
>     hosts2.add(expectedHostNameTwo);
>     hosts2.add("serverFour");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, groupComponents2, hosts2);
1637,1639c1188,1190
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
>     hostGroups.add(group2);
1641,1642c1192,1193
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
1645,1647c1196
<     assertEquals("Unexpected config update for templeton.hive.properties",
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
<       webHCatSiteProperties.get("templeton.hive.properties"));
---
>     configProcessor.doUpdateForBlueprintExport();
1649c1198,1207
<     mockSupport.verifyAll();
---
>     assertEquals("oozie property not exported correctly",
>         createExportedHostName(expectedHostGroupName), oozieSiteProperties.get("oozie.base.url"));
>     assertEquals("oozie property not exported correctly",
>         createExportedHostName(expectedHostGroupName), oozieSiteProperties.get("oozie.authentication.kerberos.principal"));
>     assertEquals("oozie property not exported correctly",
>         createExportedHostName(expectedHostGroupName), oozieSiteProperties.get("oozie.service.HadoopAccessorService.kerberos.principal"));
>     assertEquals("oozie property not exported correctly",
>         createExportedHostName(expectedHostGroupName), oozieEnvProperties.get("oozie_hostname"));
>     assertEquals("oozie property not exported correctly",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.oozie.hosts"));
1650a1209,1213
>     
>     assertFalse("oozie_existing_mysql_host should not have been present in the exported configuration",
>         oozieEnvProperties.containsKey("oozie_existing_mysql_host"));
>     assertFalse("oozie.service.JPAService.jdbc.url should not have been present in the exported configuration",
>         oozieSiteProperties.containsKey("oozie.service.JPAService.jdbc.url"));
1654,1656c1217,1218
<   public void testHiveConfigClusterUpdateDefaultValueWithMetaStoreHA() throws Exception {
<     final String expectedHostGroupName = "host_group_1";
<     final String expectedHostNameOne = "c6401.ambari.apache.org";
---
>   public void testZookeeperConfigExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
1657a1220,1223
>     final String expectedHostGroupName = "host_group_1";
>     final String expectedHostGroupNameTwo = "host_group_2";
>     final String expectedPortNumberOne = "2112";
>     final String expectedPortNumberTwo = "1221";
1659,1675c1225,1232
<     final String expectedPropertyValue =
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     Stack mockStack = mockSupport.createMock(Stack.class);
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("HIVE_METASTORE")).atLeastOnce();
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Collections.singleton(expectedHostNameOne)).atLeastOnce();
< 
<     expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("HIVE_METASTORE")).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Collections.singleton(expectedHostNameTwo)).atLeastOnce();
< 
<     mockSupport.replayAll();
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
>     Map<String, String> hbaseSiteProperties = new HashMap<String, String>();
>     Map<String, String> webHCatSiteProperties = new HashMap<String, String>();
>     Map<String, String> sliderClientProperties = new HashMap<String, String>();
>     Map<String, String> yarnSiteProperties = new HashMap<String, String>();
>     Map<String, String> kafkaBrokerProperties = new HashMap<String, String>();
>     Map<String, String> accumuloSiteProperties = new HashMap<String, String>();
1677,1678c1234,1240
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     configProperties.put("core-site", coreSiteProperties);
>     configProperties.put("hbase-site", hbaseSiteProperties);
>     configProperties.put("webhcat-site", webHCatSiteProperties);
>     configProperties.put("slider-client", sliderClientProperties);
>     configProperties.put("yarn-site", yarnSiteProperties);
>     configProperties.put("kafka-broker", kafkaBrokerProperties);
>     configProperties.put("accumulo-site", accumuloSiteProperties);
1680,1681c1242,1248
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
---
>     coreSiteProperties.put("ha.zookeeper.quorum", expectedHostName + "," + expectedHostNameTwo);
>     hbaseSiteProperties.put("hbase.zookeeper.quorum", expectedHostName + "," + expectedHostNameTwo);
>     webHCatSiteProperties.put("templeton.zookeeper.hosts", expectedHostName + "," + expectedHostNameTwo);
>     yarnSiteProperties.put("hadoop.registry.zk.quorum", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
>     sliderClientProperties.put("slider.zookeeper.quorum", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
>     kafkaBrokerProperties.put("zookeeper.connect", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
>     accumuloSiteProperties.put("instance.zookeeper.host", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
1683c1250,1251
<     configProperties.put("webhcat-site", webHCatSiteProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1686,1690c1254,1266
<     webHCatSiteProperties.put("templeton.hive.properties",
<       expectedPropertyValue);
< 
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("ZOOKEEPER_SERVER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
> 
>     Collection<String> groupComponents2 = new HashSet<String>();
>     groupComponents2.add("ZOOKEEPER_SERVER");
>     Collection<String> hosts2 = new ArrayList<String>();
>     hosts2.add(expectedHostNameTwo);
>     hosts2.add("serverFour");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, groupComponents2, hosts2);
1692,1695c1268,1270
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put("host_group_2", mockHostGroupTwo);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
>     hostGroups.add(group2);
1697,1698c1272,1273
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
1701,1706c1276
<     
<     assertEquals("Unexpected config update for templeton.hive.properties",
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
<       webHCatSiteProperties.get("templeton.hive.properties"));
< 
<     mockSupport.verifyAll();
---
>     configProcessor.doUpdateForBlueprintExport();
1707a1278,1298
>     assertEquals("zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         coreSiteProperties.get("ha.zookeeper.quorum"));
>     assertEquals("zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         hbaseSiteProperties.get("hbase.zookeeper.quorum"));
>     assertEquals("zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         webHCatSiteProperties.get("templeton.zookeeper.hosts"));
>     assertEquals("yarn-site zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
>         yarnSiteProperties.get("hadoop.registry.zk.quorum"));
>     assertEquals("slider-client zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
>         sliderClientProperties.get("slider.zookeeper.quorum"));
>     assertEquals("kafka zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
>         kafkaBrokerProperties.get("zookeeper.connect"));
>     assertEquals("accumulo-site zookeeper config not properly exported",
>         createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
>         accumuloSiteProperties.get("instance.zookeeper.host"));
1711c1302,1304
<   public void testHiveConfigClusterUpdateExportedHostGroupValue() throws Exception {
---
>   public void testKnoxSecurityConfigExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.ambari.apache.org";
1713c1306
<     final String expectedHostName = "c6401.ambari.apache.org";
---
>     final String expectedHostGroupNameTwo = "host_group_2";
1715,1717c1308,1309
<     
<     final String expectedPropertyValue =
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
---
>     Map<String, Map<String, String>> configProperties =
>         new HashMap<String, Map<String, String>>();
1719c1311,1316
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, String> coreSiteProperties =
>         new HashMap<String, String>();
>     Map<String, String> webHCatSiteProperties =
>         new HashMap<String, String>();
>     Map<String, String> oozieSiteProperties =
>         new HashMap<String, String>();
1721,1722c1318,1320
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Collections.singleton(expectedHostName)).atLeastOnce();
---
>     configProperties.put("core-site", coreSiteProperties);
>     configProperties.put("webhcat-site", webHCatSiteProperties);
>     configProperties.put("oozie-site", oozieSiteProperties);
1724c1322,1325
<     mockSupport.replayAll();
---
>     coreSiteProperties.put("hadoop.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
>     webHCatSiteProperties.put("webhcat.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
>     oozieSiteProperties.put("hadoop.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
>     oozieSiteProperties.put("oozie.service.ProxyUserService.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
1726,1727d1326
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
1729,1730d1327
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
1732d1328
<     configProperties.put("webhcat-site", webHCatSiteProperties);
1734,1736d1329
<     
<     webHCatSiteProperties.put("templeton.hive.properties",
<       expectedPropertyValue);
1738,1739c1331,1332
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1741,1743c1334,1346
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
---
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("KNOX_GATEWAY");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
> 
>     Collection<String> groupComponents2 = new HashSet<String>();
>     groupComponents2.add("KNOX_GATEWAY");
>     Collection<String> hosts2 = new ArrayList<String>();
>     hosts2.add(expectedHostNameTwo);
>     hosts2.add("serverFour");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, groupComponents2, hosts2);
1745,1746c1348,1350
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
>     hostGroups.add(group2);
1748,1751c1352,1353
<     
<     assertEquals("Unexpected config update for templeton.hive.properties",
<       "hive.metastore.local=false,hive.metastore.uris=thrift:
<       webHCatSiteProperties.get("templeton.hive.properties"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
1753c1355,1356
<     mockSupport.verifyAll();
---
>     
>     configProcessor.doUpdateForBlueprintExport();
1754a1358,1369
>     assertEquals("Knox for core-site config not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         coreSiteProperties.get("hadoop.proxyuser.knox.hosts"));
>     assertEquals("Knox config for WebHCat not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         webHCatSiteProperties.get("webhcat.proxyuser.knox.hosts"));
>     assertEquals("Knox config for Oozie not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         oozieSiteProperties.get("hadoop.proxyuser.knox.hosts"));
>     assertEquals("Knox config for Oozie not properly exported",
>         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
>         oozieSiteProperties.get("oozie.service.ProxyUserService.proxyuser.knox.hosts"));
1758c1373,1374
<   public void testStormAndKafkaConfigClusterUpdateWithoutGangliaServer() throws Exception {
---
>   public void testKafkaConfigExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
1759a1376
>     final String expectedPortNumberOne = "2112";
1761c1378,1381
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> kafkaBrokerProperties = new HashMap<String, String>();
>     configProperties.put("kafka-broker", kafkaBrokerProperties);
>     kafkaBrokerProperties.put("kafka.ganglia.metrics.host", createHostAddress(expectedHostName, expectedPortNumberOne));
1763,1764c1383,1384
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     Stack mockStack = mockSupport.createMock(Stack.class);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1766,1768c1386,1395
<     
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.<String>emptySet()).atLeastOnce();
<     expect(mockStack.getCardinality("GANGLIA_SERVER")).andReturn(new Cardinality("1")).atLeastOnce();
---
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("KAFKA_BROKER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
> 
>     Collection<String> groupComponents2 = new HashSet<String>();
>     groupComponents2.add("NAMENODE");
>     TestHostGroup group2 = new TestHostGroup("group2", groupComponents2, Collections.singleton("group2Host"));
1770c1397,1399
<     mockSupport.replayAll();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
>     hostGroups.add(group2);
1772,1773c1401,1402
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
1775,1778c1404,1405
<     Map<String, String> stormSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> kafkaBrokerProperties =
<       new HashMap<String, String>();
---
>     
>     configProcessor.doUpdateForBlueprintExport();
1780,1781c1407,1410
<     configProperties.put("storm-site", stormSiteProperties);
<     configProperties.put("kafka-broker", kafkaBrokerProperties);
---
>     assertEquals("kafka Ganglia config not properly exported",
>         createExportedHostName(expectedHostGroupName, expectedPortNumberOne),
>         kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
>   }
1783,1785c1412,1415
<     stormSiteProperties.put("worker.childopts", "localhost");
<     stormSiteProperties.put("supervisor.childopts", "localhost");
<     stormSiteProperties.put("nimbus.childopts", "localhost");
---
>   @Test
>   public void testPropertyWithUndefinedHostisExported() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedHostGroupName = "host_group_1";
1787c1417
<     kafkaBrokerProperties.put("kafka.ganglia.metrics.host", "localhost");
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
1788a1419,1420
>     Map<String, String> properties = new HashMap<String, String>();
>     configProperties.put("storm-site", properties);
1790a1423,1425
>     properties.put("storm.zookeeper.servers", expectedHostName);
>     properties.put("nimbus.childopts", "undefined");
>     properties.put("worker.childopts", "some other info, undefined, more info");
1792,1800c1427,1428
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
< 
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
< 
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     Configuration clusterConfig = new Configuration(configProperties,
>         Collections.<String, Map<String, Map<String, String>>>emptyMap());
1802,1805c1430,1435
<     
<     
<     assertEquals("worker startup settings not properly handled by cluster create",
<       "localhost", stormSiteProperties.get("worker.childopts"));
---
>     Collection<String> groupComponents = new HashSet<String>();
>     groupComponents.add("ZOOKEEPER_SERVER");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, groupComponents, hosts);
1807,1808c1437,1438
<     assertEquals("supervisor startup settings not properly handled by cluster create",
<       "localhost", stormSiteProperties.get("supervisor.childopts"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group);
1810,1811c1440,1441
<     assertEquals("nimbus startup settings not properly handled by cluster create",
<       "localhost", stormSiteProperties.get("nimbus.childopts"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor configProcessor = new BlueprintConfigurationProcessor(topology);
1813,1814c1443,1444
<     assertEquals("Kafka ganglia host property not properly handled by cluster create",
<       "localhost", kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
---
>     
>     configProcessor.doUpdateForBlueprintExport();
1816c1446,1451
<     mockSupport.verifyAll();
---
>     assertEquals("Property was incorrectly exported",
>         "%HOSTGROUP::" + expectedHostGroupName + "%", properties.get("storm.zookeeper.servers"));
>     assertEquals("Property with undefined host was incorrectly exported",
>         "undefined", properties.get("nimbus.childopts"));
>     assertEquals("Property with undefined host was incorrectly exported",
>         "some other info, undefined, more info" , properties.get("worker.childopts"));
1820,1822c1455,1459
<   public void testStormandKafkaConfigClusterUpdateWithGangliaServer() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostGroupName = "host_group_1";
---
>   public void testDoUpdateForClusterCreate_SingleHostProperty__defaultValue() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.resourcemanager.hostname", "localhost");
>     properties.put("yarn-site", typeProps);
1824c1461
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1826,1827c1463,1472
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     Stack mockStack = mockSupport.createMock(Stack.class);
---
>     Collection<String> group1Components = new HashSet<String>();
>     group1Components.add("NAMENODE");
>     group1Components.add("SECONDARY_NAMENODE");
>     group1Components.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", group1Components, Collections.singleton("testhost"));
> 
>     Collection<String> group2Components = new HashSet<String>();
>     group2Components.add("DATANODE");
>     group2Components.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", group2Components, Collections.singleton("testhost2"));
1829,1831c1474,1476
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("GANGLIA_SERVER")).atLeastOnce();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
1833c1478,1479
<     mockSupport.replayAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
1835,1836c1481,1484
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = properties.get("yarn-site").get("yarn.resourcemanager.hostname");
>     assertEquals("testhost", updatedVal);
>   }
1838,1841c1486,1492
<     Map<String, String> stormSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> kafkaBrokerProperties =
<       new HashMap<String, String>();
---
>   @Test
>   public void testDoUpdateForClusterCreate_SingleHostProperty__MissingComponent() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.resourcemanager.hostname", "localhost");
>     typeProps.put("yarn.timeline-service.address", "localhost");
>     properties.put("yarn-site", typeProps);
1843,1844c1494
<     configProperties.put("storm-site", stormSiteProperties);
<     configProperties.put("kafka-broker", kafkaBrokerProperties);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1846,1848c1496,1505
<     stormSiteProperties.put("worker.childopts", "localhost");
<     stormSiteProperties.put("supervisor.childopts", "localhost");
<     stormSiteProperties.put("nimbus.childopts", "localhost");
---
>     Collection<String> group1Components = new HashSet<String>();
>     group1Components.add("NAMENODE");
>     group1Components.add("SECONDARY_NAMENODE");
>     group1Components.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", group1Components, Collections.singleton("testhost"));
> 
>     Collection<String> group2Components = new HashSet<String>();
>     group2Components.add("DATANODE");
>     group2Components.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", group2Components, Collections.singleton("testhost2"));
1850c1507,1514
<     kafkaBrokerProperties.put("kafka.ganglia.metrics.host", "localhost");
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
> 
>     expect(stack.getCardinality("APP_TIMELINE_SERVER")).andReturn(new Cardinality("1")).anyTimes();
> 
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
1852a1517,1523
>     try {
>       updater.doUpdateForClusterCreate();
>       fail("IllegalArgumentException should have been thrown");
>     } catch (IllegalArgumentException illegalArgumentException) {
>       
>     }
>   }
1854,1855c1525,1526
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>   @Test
>   public void testDoUpdateForClusterCreate_SingleHostProperty__MultipleMatchingHostGroupsError() throws Exception {
1857,1859c1528,1532
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.resourcemanager.hostname", "localhost");
>     typeProps.put("yarn.timeline-service.address", "localhost");
>     properties.put("yarn-site", typeProps);
1861,1862c1534
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1864,1867c1536,1547
<     
<     
<     assertEquals("worker startup settings not properly handled by cluster create",
<       expectedHostName, stormSiteProperties.get("worker.childopts"));
---
>     Collection<String> group1Components = new HashSet<String>();
>     group1Components.add("NAMENODE");
>     group1Components.add("SECONDARY_NAMENODE");
>     group1Components.add("RESOURCEMANAGER");
>     group1Components.add("APP_TIMELINE_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", group1Components, Collections.singleton("testhost"));
> 
>     Collection<String> group2Components = new HashSet<String>();
>     group2Components.add("DATANODE");
>     group2Components.add("HDFS_CLIENT");
>     group2Components.add("APP_TIMELINE_SERVER");
>     TestHostGroup group2 = new TestHostGroup("group2", group2Components, Collections.singleton("testhost2"));
1869,1870c1549,1551
<     assertEquals("supervisor startup settings not properly handled by cluster create",
<       expectedHostName, stormSiteProperties.get("supervisor.childopts"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
1872,1873c1553
<     assertEquals("nimbus startup settings not properly handled by cluster create",
<       expectedHostName, stormSiteProperties.get("nimbus.childopts"));
---
>     expect(stack.getCardinality("APP_TIMELINE_SERVER")).andReturn(new Cardinality("0-1")).anyTimes();
1875,1876d1554
<     assertEquals("Kafka ganglia host property not properly handled by cluster create",
<       expectedHostName, kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
1878c1556,1564
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
> 
>     try {
>       updater.doUpdateForClusterCreate();
>       fail("IllegalArgumentException should have been thrown");
>     } catch (IllegalArgumentException illegalArgumentException) {
>       
>     }
1882,1889c1568,1569
<   public void testDoUpdateForClusterWithNameNodeHAEnabled() throws Exception {
<     final String expectedNameService = "mynameservice";
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "serverTwo";
<     final String expectedPortNum = "808080";
<     final String expectedNodeOne = "nn1";
<     final String expectedNodeTwo = "nn2";
<     final String expectedHostGroupName = "host_group_1";
---
>   public void testDoUpdateForClusterCreate_SingleHostProperty__MissingOptionalComponent() throws Exception {
>     final String expectedHostName = "localhost";
1891c1571,1574
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.timeline-service.address", expectedHostName);
>     properties.put("yarn-site", typeProps);
1893,1894c1576
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1896c1578,1587
<     Stack mockStack = mockSupport.createMock(Stack.class);
---
>     Collection<String> group1Components = new HashSet<String>();
>     group1Components.add("NAMENODE");
>     group1Components.add("SECONDARY_NAMENODE");
>     group1Components.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", group1Components, Collections.singleton("testhost"));
> 
>     Collection<String> group2Components = new HashSet<String>();
>     group2Components.add("DATANODE");
>     group2Components.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", group2Components, Collections.singleton("testhost2"));
1898,1903c1589,1591
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName)).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo)).atLeastOnce();
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("NAMENODE")).atLeastOnce();
<     expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("NAMENODE")).atLeastOnce();
<     expect(mockStack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).atLeastOnce();
<     expect(mockStack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).atLeastOnce();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
1905c1593
<     mockSupport.replayAll();
---
>     expect(stack.getCardinality("APP_TIMELINE_SERVER")).andReturn(new Cardinality("0-1")).anyTimes();
1907,1908c1595,1596
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
1910,1919c1598,1601
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hbaseSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hadoopEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> accumuloSiteProperties =
<         new HashMap<String, String>();
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("yarn-site").get("yarn.timeline-service.address");
>     assertEquals("Timeline Server config property should not have been updated", expectedHostName, updatedVal);
>   }
1920a1603,1608
>   @Test
>   public void testDoUpdateForClusterCreate_SingleHostProperty__defaultValue__WithPort() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("fs.defaultFS", "localhost:5050");
>     properties.put("core-site", typeProps);
1922,1926c1610
<     configProperties.put("hdfs-site", hdfsSiteProperties);
<     configProperties.put("hadoop-env", hadoopEnvProperties);
<     configProperties.put("core-site", coreSiteProperties);
<     configProperties.put("hbase-site", hbaseSiteProperties);
<     configProperties.put("accumulo-site", accumuloSiteProperties);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1928,1930c1612,1616
<     
<     hdfsSiteProperties.put("dfs.nameservices", expectedNameService);
<     hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice", expectedNodeOne + ", " + expectedNodeTwo);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
1931a1618,1621
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
1933,1939c1623,1625
<     
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
1941,1944c1627,1632
<     
<     
<     hdfsSiteProperties.put("dfs.secondary.http.address", "localhost:8080");
<     hdfsSiteProperties.put("dfs.namenode.secondary.http-address", "localhost:8080");
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("core-site").get("fs.defaultFS");
>     assertEquals("testhost:5050", updatedVal);
>   }
1946,1947c1634,1639
<     
<     coreSiteProperties.put("fs.defaultFS", "hdfs:
---
>   @Test
>   public void testDoUpdateForClusterCreate_MultiHostProperty__defaultValues() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("hbase.zookeeper.quorum", "localhost");
>     properties.put("hbase-site", typeProps);
1949,1950c1641
<     
<     hbaseSiteProperties.put("hbase.rootdir", "hdfs:
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1952,1953c1643,1647
<     
<     accumuloSiteProperties.put("instance.volumes", "hdfs:
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
1955,1956c1649,1657
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
1958,1960c1659,1665
<     Map<String, HostGroup> mapOfHostGroups = new LinkedHashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put("host-group-2", mockHostGroupTwo);
---
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
1962c1667,1670
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
1964,1968c1672,1676
<     
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hbase-site").get("hbase.zookeeper.quorum");
>     String[] hosts = updatedVal.split(",");
1970,1973c1678,1682
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
---
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("testhost");
>     expectedHosts.add("testhost2");
>     expectedHosts.add("testhost2a");
>     expectedHosts.add("testhost2b");
1975,1978c1684,1689
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
---
>     assertEquals(4, hosts.length);
>     for (String host : hosts) {
>       assertTrue(expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
>   }
1980,1983c1691,1696
<     
<     
<     assertEquals("Active Namenode hostname was not set correctly",
<       expectedHostName, hadoopEnvProperties.get("dfs_ha_initial_namenode_active"));
---
>   @Test
>   public void testDoUpdateForClusterCreate_MultiHostProperty__defaultValues___withPorts() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("templeton.zookeeper.hosts", "localhost:9090");
>     properties.put("webhcat-site", typeProps);
1985,1986c1698
<     assertEquals("Standby Namenode hostname was not set correctly",
<       expectedHostNameTwo, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
1988,1989c1700,1704
<     assertEquals("fs.defaultFS should not be modified by cluster update when NameNode HA is enabled.",
<                  "hdfs:
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
1991,1992c1706,1714
<     assertEquals("hbase.rootdir should not be modified by cluster update when NameNode HA is enabled.",
<       "hdfs:
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
1994,1995c1716,1733
<     assertEquals("instance.volumes should not be modified by cluster update when NameNode HA is enabled.",
<         "hdfs:
---
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
> 
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("webhcat-site").get("templeton.zookeeper.hosts");
>     String[] hosts = updatedVal.split(",");
1997c1735,1745
<     mockSupport.verifyAll();
---
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("testhost:9090");
>     expectedHosts.add("testhost2:9090");
>     expectedHosts.add("testhost2a:9090");
>     expectedHosts.add("testhost2b:9090");
> 
>     assertEquals(4, hosts.length);
>     for (String host : hosts) {
>       assertTrue(expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
2010,2039c1758,1763
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     Stack mockStack = mockSupport.createMock(Stack.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName)).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo)).atLeastOnce();
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("NAMENODE")).atLeastOnce();
<     expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("NAMENODE")).atLeastOnce();
<     expect(mockStack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).atLeastOnce();
<     expect(mockStack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).atLeastOnce();
< 
<     mockSupport.replayAll();
< 
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hbaseSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hadoopEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> accumuloSiteProperties =
<       new HashMap<String, String>();
< 
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     Map<String, String> hbaseSiteProperties = new HashMap<String, String>();
>     Map<String, String> hadoopEnvProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
>     Map<String, String> accumuloSiteProperties = new HashMap<String, String>();
2051d1774
< 
2074,2075c1797,1809
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton(expectedHostName));
> 
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("NAMENODE");
>     TestHostGroup group2 = new TestHostGroup("host-group-2", hgComponents2, Collections.singleton(expectedHostNameTwo));
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2077,2079c1811,1812
<     Map<String, HostGroup> mapOfHostGroups = new LinkedHashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put("host-group-2", mockHostGroupTwo);
---
>     expect(stack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).anyTimes();
>     expect(stack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).anyTimes();
2081c1814,1816
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2085c1820
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
---
>         expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
2087c1822
<       expectedHostNameTwo + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
---
>         expectedHostNameTwo + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
2090c1825
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
---
>         expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
2092c1827
<       expectedHostNameTwo + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
---
>         expectedHostNameTwo + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
2095c1830
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
---
>         expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
2097c1832
<       expectedHostNameTwo + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
---
>         expectedHostNameTwo + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
2101,2105c1836,1846
<     assertEquals("Active Namenode hostname was not set correctly",
<       expectedHostName, hadoopEnvProperties.get("dfs_ha_initial_namenode_active"));
< 
<     assertEquals("Standby Namenode hostname was not set correctly",
<       expectedHostNameTwo, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
---
>     
>     String activeHost = hadoopEnvProperties.get("dfs_ha_initial_namenode_active");
>     if (activeHost.equals(expectedHostName)) {
>       assertEquals("Standby Namenode hostname was not set correctly",
>           expectedHostNameTwo, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
>     } else if (activeHost.equals(expectedHostNameTwo)) {
>       assertEquals("Standby Namenode hostname was not set correctly",
>           expectedHostName, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
>     } else {
>       fail("Active Namenode hostname was not set correctly: " + activeHost);
>     }
2108c1849
<       "hdfs:
---
>         "hdfs:
2111c1852
<       "hdfs:
---
>         "hdfs:
2114,2116c1855
<       "hdfs:
< 
<     mockSupport.verifyAll();
---
>         "hdfs:
2120,2126c1859
<   public void testDoUpdateForClusterWithNameNodeHAEnabledAndActiveNodeSet() throws Exception {
<     final String expectedNameService = "mynameservice";
<     final String expectedHostName = "serverThree";
<     final String expectedHostNameTwo = "serverFour";
<     final String expectedPortNum = "808080";
<     final String expectedNodeOne = "nn1";
<     final String expectedNodeTwo = "nn2";
---
>   public void testHiveConfigClusterUpdateCustomValueSpecifyingHostNamesMetaStoreHA() throws Exception {
2129c1862,1863
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     final String expectedPropertyValue =
>         "hive.metastore.local=false,hive.metastore.uris=thrift:
2131c1865,1867
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> webHCatSiteProperties = new HashMap<String, String>();
>     configProperties.put("webhcat-site", webHCatSiteProperties);
2133c1869,1870
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, expectedHostNameTwo)).atLeastOnce();
---
>     
>     webHCatSiteProperties.put("templeton.hive.properties", expectedPropertyValue);
2135c1872
<     mockSupport.replayAll();
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2137,2138c1874,1876
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton("some-host"));
2140,2141c1878,1880
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     TestHostGroup group2 = new TestHostGroup("host_group_2", hgComponents2, Collections.singleton("some-host2"));
2143,2144c1882,1884
<     Map<String, String> hadoopEnvProperties =
<       new HashMap<String, String>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2146,2147c1886,1888
<     configProperties.put("hdfs-site", hdfsSiteProperties);
<     configProperties.put("hadoop-env", hadoopEnvProperties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2149,2151c1890,1913
<     
<     hdfsSiteProperties.put("dfs.nameservices", expectedNameService);
<     hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice", expectedNodeOne + ", " + expectedNodeTwo);
---
>     assertEquals("Unexpected config update for templeton.hive.properties",
>         expectedPropertyValue,
>         webHCatSiteProperties.get("templeton.hive.properties"));
>   }
> 
>   @Test
>   public void testHiveConfigClusterUpdateSpecifyingHostNamesHiveServer2HA() throws Exception {
>     final String expectedHostGroupName = "host_group_1";
> 
>     final String expectedPropertyValue =
>         "c6401.ambari.apache.org";
> 
>     final String expectedMetaStoreURIs = "thrift:
> 
>     Map<String, Map<String, String>> configProperties =
>         new HashMap<String, Map<String, String>>();
> 
>     Map<String, String> hiveEnvProperties =
>         new HashMap<String, String>();
>     Map<String, String> hiveSiteProperties =
>         new HashMap<String, String>();
> 
>     configProperties.put("hive-env", hiveEnvProperties);
>     configProperties.put("hive-site", hiveSiteProperties);
2154,2159c1916,1917
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
---
>     hiveEnvProperties.put("hive_hostname",
>         expectedPropertyValue);
2161a1920,1921
>     hiveSiteProperties.put("hive.server2.support.dynamic.service.discovery", "true");
> 
2163,2164d1922
<     hadoopEnvProperties.put("dfs_ha_initial_namenode_active", expectedHostName);
<     hadoopEnvProperties.put("dfs_ha_initial_namenode_standby", expectedHostNameTwo);
2166,2167c1924
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     hiveSiteProperties.put("hive.metastore.uris", expectedMetaStoreURIs);
2169,2170c1926
<     Map<String, HostGroup> mapOfHostGroups = new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName,mockHostGroupOne);
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2172c1928,1930
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("HIVE_SERVER");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton("some-host"));
2174,2178c1932,1934
<     
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("HIVE_SERVER");
>     TestHostGroup group2 = new TestHostGroup("host_group_2", hgComponents2, Collections.singleton("some-host2"));
2180,2183c1936,1938
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2185,2188c1940
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
---
>     expect(stack.getCardinality("HIVE_SERVER")).andReturn(new Cardinality("1+")).anyTimes();
2190,2194c1942,1944
<     
<     
<     
<     assertEquals("Active Namenode hostname was not set correctly",
<       expectedHostName, hadoopEnvProperties.get("dfs_ha_initial_namenode_active"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2196,2197c1946,1948
<     assertEquals("Standby Namenode hostname was not set correctly",
<       expectedHostNameTwo, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
---
>     assertEquals("Unexpected config update for hive_hostname",
>         expectedPropertyValue,
>         hiveEnvProperties.get("hive_hostname"));
2199c1950,1952
<     mockSupport.verifyAll();
---
>     assertEquals("Unexpected config update for hive.metastore.uris",
>         expectedMetaStoreURIs,
>         hiveSiteProperties.get("hive.metastore.uris"));
2203,2209c1956,1964
<   public void testDoNameNodeHighAvailabilityUpdateWithHAEnabled() throws Exception {
<     final String expectedNameService = "mynameservice";
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedNodeOne = "nn1";
<     final String expectedNodeTwo = "nn2";
<     final String expectedHostGroupName = "host_group_1";
---
>   public void testHiveConfigClusterUpdateUsingExportedNamesHiveServer2HA() throws Exception {
>     final String expectedHostGroupNameOne = "host_group_1";
>     final String expectedHostGroupNameTwo = "host_group_2";
> 
>     final String expectedHostNameOne =
>         "c6401.ambari.apache.org";
> 
>     final String expectedHostNameTwo =
>         "c6402.ambari.apache.org";
2211d1965
<     EasyMockSupport mockSupport = new EasyMockSupport();
2213c1967,1969
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     
>     
>     final String inputMetaStoreURIs = "thrift:
2215,2216c1971
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
---
>     final String expectedMetaStoreURIs = "thrift:
2218d1972
<     mockSupport.replayAll();
2221c1975
<       new HashMap<String, Map<String, String>>();
---
>         new HashMap<String, Map<String, String>>();
2223,2228c1977,1980
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hbaseSiteProperties =
<       new HashMap<String, String>();
---
>     Map<String, String> hiveEnvProperties =
>         new HashMap<String, String>();
>     Map<String, String> hiveSiteProperties =
>         new HashMap<String, String>();
2229a1982,1983
>     configProperties.put("hive-env", hiveEnvProperties);
>     configProperties.put("hive-site", hiveSiteProperties);
2231,2233c1985,1987
<     configProperties.put("hdfs-site", hdfsSiteProperties);
<     configProperties.put("core-site", coreSiteProperties);
<     configProperties.put("hbase-site", hbaseSiteProperties);
---
>     
>     hiveEnvProperties.put("hive_hostname",
>         expectedHostNameOne);
2235a1990
>     hiveSiteProperties.put("hive.server2.support.dynamic.service.discovery", "true");
2237,2238c1992
<     hdfsSiteProperties.put("dfs.nameservices", expectedNameService);
<     hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice", expectedNodeOne + ", " + expectedNodeTwo);
---
>     
2239a1994
>     hiveSiteProperties.put("hive.metastore.uris", inputMetaStoreURIs);
2241,2247c1996
<     
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne, expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo, expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne, expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo, expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne, expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo, expectedHostName + ":" + expectedPortNum);
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2249,2250c1998,2001
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("HIVE_SERVER");
>     hgComponents.add("HIVE_METASTORE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupNameOne, hgComponents, Collections.singleton(expectedHostNameOne));
2252,2253c2003,2006
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("HIVE_SERVER");
>     hgComponents2.add("HIVE_METASTORE");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, hgComponents2, Collections.singleton(expectedHostNameTwo));
2255,2258c2008,2010
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2260,2263c2012
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
---
>     expect(stack.getCardinality("HIVE_SERVER")).andReturn(new Cardinality("1+")).anyTimes();
2265,2268c2014,2016
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2270c2018,2020
<     mockSupport.verifyAll();
---
>     assertEquals("Unexpected config update for hive_hostname",
>         expectedHostNameOne,
>         hiveEnvProperties.get("hive_hostname"));
2271a2022,2024
>     assertEquals("Unexpected config update for hive.metastore.uris",
>         expectedMetaStoreURIs,
>         hiveSiteProperties.get("hive.metastore.uris"));
2275,2277c2028,2034
<   public void testDoNameNodeHighAvailabilityUpdateWithHAEnabledNameServicePropertiesIncluded() throws Exception {
<     final String expectedNameService = "mynameservice";
<     final String expectedHostName = "c6401.apache.ambari.org";
---
>   public void testHiveConfigClusterUpdateDefaultValueWithMetaStoreHA() throws Exception {
>     final String expectedHostGroupName = "host_group_1";
>     final String expectedHostNameOne = "c6401.ambari.apache.org";
>     final String expectedHostNameTwo = "c6402.ambari.apache.org";
> 
>     final String expectedPropertyValue =
>         "hive.metastore.local=false,hive.metastore.uris=thrift:
2279c2036,2037
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> configProperties =
>         new HashMap<String, Map<String, String>>();
2281c2039,2040
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     Map<String, String> webHCatSiteProperties =
>         new HashMap<String, String>();
2283c2042
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
---
>     configProperties.put("webhcat-site", webHCatSiteProperties);
2285c2044,2046
<     mockSupport.replayAll();
---
>     
>     webHCatSiteProperties.put("templeton.hive.properties",
>         expectedPropertyValue);
2287,2288c2048
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2290,2295c2050,2052
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hbaseSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> accumuloSiteProperties =
<         new HashMap<String, String>();
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("HIVE_METASTORE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton(expectedHostNameOne));
2296a2054,2056
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("HIVE_METASTORE");
>     TestHostGroup group2 = new TestHostGroup("host_group_2", hgComponents2, Collections.singleton(expectedHostNameTwo));
2298,2300c2058,2064
<     configProperties.put("core-site", coreSiteProperties);
<     configProperties.put("hbase-site", hbaseSiteProperties);
<     configProperties.put("accumulo-site", accumuloSiteProperties);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
> 
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2303,2305d2066
<     coreSiteProperties.put("fs.defaultFS", "hdfs:
<     
<     hbaseSiteProperties.put("hbase.rootdir", "hdfs:
2307c2068,2080
<     accumuloSiteProperties.put("instance.volumes", "hdfs:
---
>     assertEquals("Unexpected config update for templeton.hive.properties",
>         "hive.metastore.local=false,hive.metastore.uris=thrift:
>         expectedHostNameTwo + ":9933" + "," + "hive.metastore.sasl.enabled=false",
>         webHCatSiteProperties.get("templeton.hive.properties"));
>   }
> 
>   @Test
>   public void testOozieConfigClusterUpdateHAEnabledSpecifyingHostNamesDirectly() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.ambari.apache.org";
>     final String expectedExternalHost = "c6408.ambari.apache.org";
>     final String expectedHostGroupName = "host_group_1";
>     final String expectedHostGroupNameTwo = "host_group_2";
2309,2310c2082,2085
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> oozieSiteProperties = new HashMap<String, String>();
>     Map<String, String> oozieEnvProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
2312,2313c2087,2095
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>     configProperties.put("oozie-site", oozieSiteProperties);
>     configProperties.put("oozie-env", oozieEnvProperties);
>     configProperties.put("hive-env", oozieEnvProperties);
>     configProperties.put("core-site", coreSiteProperties);
> 
>     oozieSiteProperties.put("oozie.base.url", expectedHostName);
>     oozieSiteProperties.put("oozie.authentication.kerberos.principal", expectedHostName);
>     oozieSiteProperties.put("oozie.service.HadoopAccessorService.kerberos.principal", expectedHostName);
>     oozieSiteProperties.put("oozie.service.JPAService.jdbc.url", "jdbc:mysql:
2316,2321c2098,2099
<     assertEquals("Property containing an HA nameservice (fs.defaultFS), was not correctly exported by the processor",
<       "hdfs:
<     assertEquals("Property containing an HA nameservice (hbase.rootdir), was not correctly exported by the processor",
<       "hdfs:
<     assertEquals("Property containing an HA nameservice (instance.volumes), was not correctly exported by the processor",
<         "hdfs:
---
>     oozieSiteProperties.put("oozie.services.ext",
>         "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService");
2323d2100
<     mockSupport.verifyAll();
2325c2102,2103
<   }
---
>     oozieEnvProperties.put("oozie_hostname", expectedHostName);
>     oozieEnvProperties.put("oozie_existing_mysql_host", expectedExternalHost);
2327,2329c2105,2118
<   @Test
<   public void testDoNameNodeHighAvailabilityUpdateWithHANotEnabled() throws Exception {
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     coreSiteProperties.put("hadoop.proxyuser.oozie.hosts", expectedHostName + "," + expectedHostNameTwo);
> 
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("OOZIE_SERVER");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton("host1"));
> 
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("OOZIE_SERVER");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, hgComponents2, Collections.singleton("host2"));
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2331c2120
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     expect(stack.getCardinality("OOZIE_SERVER")).andReturn(new Cardinality("1+")).anyTimes();
2333c2122,2124
<     mockSupport.replayAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2335,2336c2126,2136
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     assertEquals("oozie property not updated correctly",
>         expectedHostName, oozieSiteProperties.get("oozie.base.url"));
>     assertEquals("oozie property not updated correctly",
>         expectedHostName, oozieSiteProperties.get("oozie.authentication.kerberos.principal"));
>     assertEquals("oozie property not updated correctly",
>         expectedHostName, oozieSiteProperties.get("oozie.service.HadoopAccessorService.kerberos.principal"));
>     assertEquals("oozie property not updated correctly",
>         expectedHostName, oozieEnvProperties.get("oozie_hostname"));
>     assertEquals("oozie property not updated correctly",
>         expectedHostName + "," + expectedHostNameTwo, coreSiteProperties.get("hadoop.proxyuser.oozie.hosts"));
>   }
2338,2339c2138,2143
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
---
>   @Test
>   public void testYarnHighAvailabilityConfigClusterUpdateSpecifyingHostNamesDirectly() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
>     final String expectedPortNum = "808080";
>     final String expectedHostGroupName = "host_group_1";
>     final String expectedHostGroupNameTwo = "host_group_2";
2341c2145,2147
<     configProperties.put("hdfs-site", hdfsSiteProperties);
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> yarnSiteProperties = new HashMap<String, String>();
>     configProperties.put("yarn-site", yarnSiteProperties);
2343a2150,2160
>     yarnSiteProperties.put("yarn.log.server.url", "http:
>     yarnSiteProperties.put("yarn.resourcemanager.hostname", expectedHostName);
>     yarnSiteProperties.put("yarn.resourcemanager.resource-tracker.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.webapp.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.scheduler.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.admin.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.webapp.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.timeline-service.webapp.https.address", expectedHostName + ":" + expectedPortNum);
>     yarnSiteProperties.put("yarn.resourcemanager.ha.enabled", "true");
2345,2346c2162,2167
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("RESOURCEMANAGER");
>     hgComponents.add("APP_TIMELINE_SERVER");
>     hgComponents.add("HISTORYSERVER");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton(expectedHostName));
2348,2349c2169,2171
<     assertEquals("Incorrect initial state for hdfs-site config",
<       0, hdfsSiteProperties.size());
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("RESOURCEMANAGER");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, hgComponents2, Collections.singleton("host2"));
2351,2352c2173,2175
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2354,2355c2177
<     assertEquals("Incorrect state for hdsf-site config after HA call in non-HA environment, should be zero",
<       0, hdfsSiteProperties.size());
---
>     expect(stack.getCardinality("RESOURCEMANAGER")).andReturn(new Cardinality("1-2")).anyTimes();
2357c2179,2181
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2358a2183,2203
>     
>     assertEquals("Yarn Log Server URL was incorrectly updated",
>         "http:
>     assertEquals("Yarn ResourceManager hostname was incorrectly exported",
>         expectedHostName, yarnSiteProperties.get("yarn.resourcemanager.hostname"));
>     assertEquals("Yarn ResourceManager tracker address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.resource-tracker.address"));
>     assertEquals("Yarn ResourceManager webapp address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.webapp.address"));
>     assertEquals("Yarn ResourceManager scheduler address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.scheduler.address"));
>     assertEquals("Yarn ResourceManager address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.address"));
>     assertEquals("Yarn ResourceManager admin address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.admin.address"));
>     assertEquals("Yarn ResourceManager timeline-service address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.timeline-service.address"));
>     assertEquals("Yarn ResourceManager timeline webapp address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.timeline-service.webapp.address"));
>     assertEquals("Yarn ResourceManager timeline webapp HTTPS address was incorrectly updated",
>         createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.timeline-service.webapp.https.address"));
2362,2364c2207
<   public void testDoNameNodeHighAvailabilityUpdateWithHAEnabledMultipleServices() throws Exception {
<     final String expectedNameServiceOne = "mynameserviceOne";
<     final String expectedNameServiceTwo = "mynameserviceTwo";
---
>   public void testHDFSConfigClusterUpdateQuorumJournalURLSpecifyingHostNamesDirectly() throws Exception {
2367d2209
< 
2369,2370d2210
<     final String expectedNodeOne = "nn1";
<     final String expectedNodeTwo = "nn2";
2371a2212,2214
>     final String expectedHostGroupNameTwo = "host_group_2";
>     final String expectedQuorumJournalURL = "qjournal:
>         createHostAddress(expectedHostNameTwo, expectedPortNum) + "/mycluster";
2373c2216,2218
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     configProperties.put("hdfs-site", hdfsSiteProperties);
2375c2220,2221
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     
>     
2377,2378c2223
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostNameOne, expectedHostNameTwo, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
---
>     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", expectedQuorumJournalURL);
2380c2225,2228
<     mockSupport.replayAll();
---
>     Configuration clusterConfig = new Configuration(configProperties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton("host1"));
2382,2383c2230,2232
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, hgComponents2, Collections.singleton("host2"));
2385,2386c2234,2236
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2388c2238,2240
<     configProperties.put("hdfs-site", hdfsSiteProperties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
>     updater.doUpdateForClusterCreate();
2390a2243,2246
>     assertEquals("HDFS HA shared edits directory property should not have been modified, since FQDNs were specified.",
>         expectedQuorumJournalURL,
>         hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
>   }
2392,2394c2248,2253
<     hdfsSiteProperties.put("dfs.nameservices", expectedNameServiceOne + "," + expectedNameServiceTwo);
<     hdfsSiteProperties.put("dfs.ha.namenodes." + expectedNameServiceOne, expectedNodeOne + ", " + expectedNodeTwo);
<     hdfsSiteProperties.put("dfs.ha.namenodes." + expectedNameServiceTwo, expectedNodeOne + ", " + expectedNodeTwo);
---
>   @Test
>   public void testDoUpdateForClusterCreate_MultiHostProperty__defaultValues___YAML() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("storm.zookeeper.servers", "['localhost']");
>     properties.put("storm-site", typeProps);
2395a2255
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2397,2403c2257,2261
<     
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeOne, expectedHostNameOne + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeTwo, expectedHostNameOne + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeOne, expectedHostNameOne + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeTwo, expectedHostNameOne + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeOne, expectedHostNameOne + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeTwo, expectedHostNameOne + ":" + expectedPortNum);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2405,2411c2263,2279
<     
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeOne, expectedHostNameTwo + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeTwo, expectedHostNameTwo + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeOne, expectedHostNameTwo + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeTwo, expectedHostNameTwo + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeOne, expectedHostNameTwo + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeTwo, expectedHostNameTwo + ":" + expectedPortNum);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
> 
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
2412a2281,2284
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
2414,2415d2285
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
2417,2418c2287,2288
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2419a2290,2293
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("storm-site").get("storm.zookeeper.servers");
>     assertTrue(updatedVal.startsWith("["));
>     assertTrue(updatedVal.endsWith("]"));
2421,2424c2295
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceOne + "." + expectedNodeTwo));
---
>     updatedVal = updatedVal.replaceAll("[\\[\\]]", "");
2426,2429c2297
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceOne + "." + expectedNodeTwo));
---
>     String[] hosts = updatedVal.split(",");
2431,2434c2299,2303
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceOne + "." + expectedNodeTwo));
---
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("'testhost'");
>     expectedHosts.add("'testhost2'");
>     expectedHosts.add("'testhost2a'");
>     expectedHosts.add("'testhost2b'");
2435a2305,2310
>     assertEquals(4, hosts.length);
>     for (String host : hosts) {
>       assertTrue(expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
>   }
2437,2441c2312,2317
<     
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameServiceTwo + "." + expectedNodeTwo));
---
>   @Test
>   public void testDoUpdateForClusterCreate_MProperty__defaultValues() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("hbase_master_heapsize", "512m");
>     properties.put("hbase-env", typeProps);
2443,2446c2319
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameServiceTwo + "." + expectedNodeTwo));
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2448,2451c2321,2334
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeOne));
<     assertEquals("HTTPS address HA property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameServiceTwo + "." + expectedNodeTwo));
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
> 
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2453c2336,2337
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2454a2339,2341
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hbase-env").get("hbase_master_heapsize");
>     assertEquals("512m", updatedVal);
2458,2463c2345,2349
<   public void testIsNameNodeHAEnabled() throws Exception {
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>   public void testDoUpdateForClusterCreate_MProperty__missingM() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("hbase_master_heapsize", "512");
>     properties.put("hbase-env", typeProps);
2465,2466c2351
<     assertFalse("Incorrect HA detection, hdfs-site not available",
<       configProcessor.isNameNodeHAEnabled());
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2468,2469c2353,2357
<     Map<String, String> hdfsSiteMap = new HashMap<String, String>();
<     configProperties.put("hdfs-site", hdfsSiteMap);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2471,2472c2359,2362
<     assertFalse("Incorrect HA detection, HA flag not enabled",
<       configProcessor.isNameNodeHAEnabled());
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2474c2364,2366
<     hdfsSiteMap.put("dfs.nameservices", "myTestNameService");
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2476,2477c2368,2369
<     assertTrue("Incorrect HA detection, HA was enabled",
<       configProcessor.isNameNodeHAEnabled());
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2478a2371,2373
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hbase-env").get("hbase_master_heapsize");
>     assertEquals("512m", updatedVal);
2482,2485c2377,2381
<   public void testParseNameServices() throws Exception {
<     Map<String, String> hdfsSiteConfigMap =
<       new HashMap<String, String>();
<     hdfsSiteConfigMap.put("dfs.nameservices", "serviceOne");
---
>   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.resourcemanager.hostname", "%HOSTGROUP::group1%");
>     properties.put("yarn-site", typeProps);
2487,2488c2383
<     
<     String[] result = BlueprintConfigurationProcessor.parseNameServices(hdfsSiteConfigMap);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2490,2495c2385,2389
<     assertNotNull("Resulting array was null",
<       result);
<     assertEquals("Incorrect array size",
<       1, result.length);
<     assertEquals("Incorrect value for returned name service",
<       "serviceOne", result[0]);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2497,2498c2391,2394
<     
<     hdfsSiteConfigMap.put("dfs.nameservices", " serviceTwo, serviceThree, serviceFour");
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2500c2396,2398
<     String[] resultTwo = BlueprintConfigurationProcessor.parseNameServices(hdfsSiteConfigMap);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2502,2511c2400,2405
<     assertNotNull("Resulting array was null",
<       resultTwo);
<     assertEquals("Incorrect array size",
<       3, resultTwo.length);
<     assertEquals("Incorrect value for returned name service",
<       "serviceTwo", resultTwo[0]);
<     assertEquals("Incorrect value for returned name service",
<       "serviceThree", resultTwo[1]);
<     assertEquals("Incorrect value for returned name service",
<       "serviceFour", resultTwo[2]);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
> 
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("yarn-site").get("yarn.resourcemanager.hostname");
>     assertEquals("testhost", updatedVal);
2515,2530c2409,2413
<   public void testParseNameNodes() throws Exception {
<     final String expectedServiceName = "serviceOne";
<     Map<String, String> hdfsSiteConfigMap =
<       new HashMap<String, String>();
<     hdfsSiteConfigMap.put("dfs.ha.namenodes." + expectedServiceName, "node1");
< 
<     
<     String[] result =
<       BlueprintConfigurationProcessor.parseNameNodes(expectedServiceName, hdfsSiteConfigMap);
< 
<     assertNotNull("Resulting array was null",
<       result);
<     assertEquals("Incorrect array size",
<       1, result.length);
<     assertEquals("Incorrect value for returned name nodes",
<       "node1", result[0]);
---
>   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue_UsingMinusSymbolInHostGroupName() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.resourcemanager.hostname", "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%");
>     properties.put("yarn-site", typeProps);
2532,2533c2415
<     
<     hdfsSiteConfigMap.put("dfs.ha.namenodes." + expectedServiceName, " nodeSeven, nodeEight, nodeNine");
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2535,2536c2417,2421
<     String[] resultTwo =
<       BlueprintConfigurationProcessor.parseNameNodes(expectedServiceName, hdfsSiteConfigMap);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-6", hgComponents, Collections.singleton("testhost"));
2538,2547c2423,2433
<     assertNotNull("Resulting array was null",
<       resultTwo);
<     assertEquals("Incorrect array size",
<       3, resultTwo.length);
<     assertEquals("Incorrect value for returned name node",
<       "nodeSeven", resultTwo[0]);
<     assertEquals("Incorrect value for returned name node",
<       "nodeEight", resultTwo[1]);
<     assertEquals("Incorrect value for returned name node",
<       "nodeNine", resultTwo[2]);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
> 
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2548a2435,2437
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("yarn-site").get("yarn.resourcemanager.hostname");
>     assertEquals("testhost", updatedVal);
2552,2555c2441,2445
<   public void testYarnConfigExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedHostGroupName = "host_group_1";
---
>   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue_WithPort_UsingMinusSymbolInHostGroupName() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("yarn.resourcemanager.hostname", "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%:2180");
>     properties.put("yarn-site", typeProps);
2557c2447
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2559c2449,2453
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-6", hgComponents, Collections.singleton("testhost"));
2561,2562c2455,2458
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2564c2460,2462
<     mockSupport.replayAll();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2566,2567c2464,2465
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2569,2570c2467,2470
<     Map<String, String> yarnSiteProperties =
<       new HashMap<String, String>();
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("yarn-site").get("yarn.resourcemanager.hostname");
>     assertEquals("testhost:2180", updatedVal);
>   }
2572c2472,2477
<     configProperties.put("yarn-site", yarnSiteProperties);
---
>   @Test
>   public void testDoUpdateForClusterCreate_SingleHostProperty__exportedValue__WithPort() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("fs.defaultFS", "%HOSTGROUP::group1%:5050");
>     properties.put("core-site", typeProps);
2574,2584c2479
<     
<     yarnSiteProperties.put("yarn.log.server.url", "http:
<     yarnSiteProperties.put("yarn.resourcemanager.hostname", expectedHostName);
<     yarnSiteProperties.put("yarn.resourcemanager.resource-tracker.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.webapp.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.scheduler.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.admin.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.webapp.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.webapp.https.address", expectedHostName + ":" + expectedPortNum);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2586,2587c2481,2485
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2589,2590c2487,2490
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2592,2611c2492,2494
<     assertEquals("Yarn Log Server URL was incorrectly exported",
<       "http:
<     assertEquals("Yarn ResourceManager hostname was incorrectly exported",
<       createExportedHostName(expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.hostname"));
<     assertEquals("Yarn ResourceManager tracker address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.resource-tracker.address"));
<     assertEquals("Yarn ResourceManager webapp address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.webapp.address"));
<     assertEquals("Yarn ResourceManager scheduler address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.scheduler.address"));
<     assertEquals("Yarn ResourceManager address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.address"));
<     assertEquals("Yarn ResourceManager admin address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.admin.address"));
<     assertEquals("Yarn ResourceManager timeline-service address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.timeline-service.address"));
<     assertEquals("Yarn ResourceManager timeline webapp address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.timeline-service.webapp.address"));
<     assertEquals("Yarn ResourceManager timeline webapp HTTPS address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.timeline-service.webapp.https.address"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2613c2496,2497
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2614a2499,2501
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("core-site").get("fs.defaultFS");
>     assertEquals("testhost:5050", updatedVal);
2618,2622c2505,2509
<   public void testYarnHighAvailabilityConfigClusterUpdateSpecifyingHostNamesDirectly() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedHostGroupName = "host_group_1";
<     final String expectedHostGroupNameTwo = "host_group_2";
---
>   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("hbase.zookeeper.quorum", "%HOSTGROUP::group1%,%HOSTGROUP::group2%");
>     properties.put("hbase-site", typeProps);
2624c2511
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2626,2627c2513,2517
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2629c2519,2527
<     Stack mockStack = mockSupport.createMock(Stack.class);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
2631,2637c2529,2535
<     Set<String> setOfComponents = new HashSet<String>();
<     setOfComponents.add("RESOURCEMANAGER");
<     setOfComponents.add("APP_TIMELINE_SERVER");
<     setOfComponents.add("HISTORYSERVER");
<     expect(mockHostGroupOne.getComponents()).andReturn(setOfComponents).atLeastOnce();
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Collections.singleton(expectedHostName)).atLeastOnce();
<     expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("RESOURCEMANAGER")).atLeastOnce();
---
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
2639,2641c2537,2540
<     expect(mockStack.getCardinality("RESOURCEMANAGER")).andReturn(new Cardinality("1-2")).atLeastOnce();
<     
<     
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
2643c2542,2543
<     mockSupport.replayAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2645,2646c2545,2547
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hbase-site").get("hbase.zookeeper.quorum");
>     String[] hosts = updatedVal.split(",");
2648,2649c2549,2553
<     Map<String, String> yarnSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("testhost");
>     expectedHosts.add("testhost2");
>     expectedHosts.add("testhost2a");
>     expectedHosts.add("testhost2b");
2651c2555,2560
<     configProperties.put("yarn-site", yarnSiteProperties);
---
>     assertEquals(4, hosts.length);
>     for (String host : hosts) {
>       assertTrue(expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
>   }
2653,2664c2562,2567
<     
<     yarnSiteProperties.put("yarn.log.server.url", "http:
<     yarnSiteProperties.put("yarn.resourcemanager.hostname", expectedHostName);
<     yarnSiteProperties.put("yarn.resourcemanager.resource-tracker.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.webapp.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.scheduler.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.admin.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.webapp.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.webapp.https.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.ha.enabled", "true");
---
>   @Test
>   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues___withPorts() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("templeton.zookeeper.hosts", "%HOSTGROUP::group1%:9090,%HOSTGROUP::group2%:9091");
>     properties.put("webhcat-site", typeProps);
2666,2667c2569
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2669,2671c2571,2575
<     Map<String, HostGroup> mapOfHostGroups = new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put(expectedHostGroupNameTwo, mockHostGroupTwo);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2673c2577,2585
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, mockStack);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
2675,2695c2587,2601
<     
<     assertEquals("Yarn Log Server URL was incorrectly updated",
<       "http:
<     assertEquals("Yarn ResourceManager hostname was incorrectly exported",
<       expectedHostName, yarnSiteProperties.get("yarn.resourcemanager.hostname"));
<     assertEquals("Yarn ResourceManager tracker address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.resource-tracker.address"));
<     assertEquals("Yarn ResourceManager webapp address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.webapp.address"));
<     assertEquals("Yarn ResourceManager scheduler address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.scheduler.address"));
<     assertEquals("Yarn ResourceManager address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.address"));
<     assertEquals("Yarn ResourceManager admin address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.resourcemanager.admin.address"));
<     assertEquals("Yarn ResourceManager timeline-service address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.timeline-service.address"));
<     assertEquals("Yarn ResourceManager timeline webapp address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.timeline-service.webapp.address"));
<     assertEquals("Yarn ResourceManager timeline webapp HTTPS address was incorrectly updated",
<       createHostAddress(expectedHostName, expectedPortNum), yarnSiteProperties.get("yarn.timeline-service.webapp.https.address"));
---
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
> 
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
> 
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2697c2603,2611
<     mockSupport.verifyAll();
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("webhcat-site").get("templeton.zookeeper.hosts");
>     String[] hosts = updatedVal.split(",");
> 
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("testhost:9090");
>     expectedHosts.add("testhost2:9091");
>     expectedHosts.add("testhost2a:9091");
>     expectedHosts.add("testhost2b:9091");
2698a2613,2617
>     assertEquals(4, hosts.length);
>     for (String host : hosts) {
>       assertTrue(expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
2702,2705c2621,2643
<   public void testYarnConfigExportedWithDefaultZeroHostAddress() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedHostGroupName = "host_group_1";
---
>   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues___withPorts_UsingMinusSymbolInHostGroupName() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("ha.zookeeper.quorum", "%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-6%:2181,%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-5%:2181,%HOSTGROUP::os-amb-r6-secha-1427972156-hbaseha-3-7%:2181");
>     properties.put("core-site", typeProps);
> 
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
> 
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-6", hgComponents, Collections.singleton("testhost"));
> 
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-5", hgComponents2, hosts2);
2707c2645,2651
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("os-amb-r6-secha-1427972156-hbaseha-3-7", hgComponents3, hosts3);
2709c2653,2656
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
2711,2712c2658,2659
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2714c2661,2663
<     mockSupport.replayAll();
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("core-site").get("ha.zookeeper.quorum");
>     String[] hosts = updatedVal.split(",");
2716,2717c2665,2671
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("testhost:2181");
>     expectedHosts.add("testhost2:2181");
>     expectedHosts.add("testhost2a:2181");
>     expectedHosts.add("testhost2b:2181");
>     expectedHosts.add("testhost3:2181");
>     expectedHosts.add("testhost3a:2181");
2719,2720c2673,2678
<     Map<String, String> yarnSiteProperties =
<       new HashMap<String, String>();
---
>     assertEquals(6, hosts.length);
>     for (String host : hosts) {
>       assertTrue("Expected host :" + host + "was not included in the multi-server list in this property.", expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
>   }
2722c2680,2683
<     configProperties.put("yarn-site", yarnSiteProperties);
---
>   @Test
>   public void testDoUpdateForClusterCreate_MultiHostProperty_exportedValues_withPorts_singleHostValue() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> yarnSiteConfig = new HashMap<String, String>();
2724,2734c2685,2686
<     
<     yarnSiteProperties.put("yarn.log.server.url", "http:
<     yarnSiteProperties.put("yarn.resourcemanager.hostname", expectedHostName);
<     yarnSiteProperties.put("yarn.resourcemanager.resource-tracker.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.webapp.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.scheduler.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.resourcemanager.admin.address", expectedHostName + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.address", "0.0.0.0" + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.webapp.address", "0.0.0.0" + ":" + expectedPortNum);
<     yarnSiteProperties.put("yarn.timeline-service.webapp.https.address", "0.0.0.0" + ":" + expectedPortNum);
---
>     yarnSiteConfig.put("hadoop.registry.zk.quorum", "%HOSTGROUP::host_group_1%:2181");
>     properties.put("yarn-site", yarnSiteConfig);
2736,2737c2688
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2739,2740c2690,2694
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("host_group_1", hgComponents, Collections.singleton("testhost"));
2742,2761c2696,2697
<     assertEquals("Yarn Log Server URL was incorrectly exported",
<       "http:
<     assertEquals("Yarn ResourceManager hostname was incorrectly exported",
<       createExportedHostName(expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.hostname"));
<     assertEquals("Yarn ResourceManager tracker address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.resource-tracker.address"));
<     assertEquals("Yarn ResourceManager webapp address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.webapp.address"));
<     assertEquals("Yarn ResourceManager scheduler address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.scheduler.address"));
<     assertEquals("Yarn ResourceManager address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.address"));
<     assertEquals("Yarn ResourceManager admin address was incorrectly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), yarnSiteProperties.get("yarn.resourcemanager.admin.address"));
<     assertEquals("Yarn ResourceManager timeline-service address was incorrectly exported",
<       "0.0.0.0" + ":" + expectedPortNum, yarnSiteProperties.get("yarn.timeline-service.address"));
<     assertEquals("Yarn ResourceManager timeline webapp address was incorrectly exported",
<       "0.0.0.0" + ":" + expectedPortNum, yarnSiteProperties.get("yarn.timeline-service.webapp.address"));
<     assertEquals("Yarn ResourceManager timeline webapp HTTPS address was incorrectly exported",
<       "0.0.0.0" + ":" + expectedPortNum, yarnSiteProperties.get("yarn.timeline-service.webapp.https.address"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
2763c2699,2700
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2764a2702,2704
>     updater.doUpdateForClusterCreate();
>     assertEquals("Multi-host property with single host value was not correctly updated for cluster create.",
>       "testhost:2181", topology.getConfiguration().getFullProperties().get("yarn-site").get("hadoop.registry.zk.quorum"));
2768,2780c2708,2712
<   public void testHDFSConfigExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedHostGroupName = "host_group_1";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
< 
<     mockSupport.replayAll();
---
>   public void testDoUpdateForClusterCreate_MultiHostProperty__exportedValues___YAML() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("storm.zookeeper.servers", "['%HOSTGROUP::group1%:9090','%HOSTGROUP::group2%:9091']");
>     properties.put("storm-site", typeProps);
2782,2783c2714
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2785,2786c2716,2720
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("ZOOKEEPER_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2788,2789c2722,2730
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_SERVER");
>     Set<String> hosts2 = new HashSet<String>();
>     hosts2.add("testhost2");
>     hosts2.add("testhost2a");
>     hosts2.add("testhost2b");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, hosts2);
2791,2792c2732,2738
<     Map<String, String> hbaseSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> hgComponents3 = new HashSet<String>();
>     hgComponents2.add("HDFS_CLIENT");
>     hgComponents2.add("ZOOKEEPER_CLIENT");
>     Set<String> hosts3 = new HashSet<String>();
>     hosts3.add("testhost3");
>     hosts3.add("testhost3a");
>     TestHostGroup group3 = new TestHostGroup("group3", hgComponents3, hosts3);
2794,2795c2740,2743
<     Map<String, String> accumuloSiteProperties =
<         new HashMap<String, String>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
>     hostGroups.add(group3);
2797,2800c2745,2746
<     configProperties.put("hdfs-site", hdfsSiteProperties);
<     configProperties.put("core-site", coreSiteProperties);
<     configProperties.put("hbase-site", hbaseSiteProperties);
<     configProperties.put("accumulo-site", accumuloSiteProperties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2801a2748,2751
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("storm-site").get("storm.zookeeper.servers");
>     assertTrue(updatedVal.startsWith("["));
>     assertTrue(updatedVal.endsWith("]"));
2803,2812c2753
<     hdfsSiteProperties.put("dfs.http.address", expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.https.address", expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.http-address", expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.https-address", expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.secondary.http.address", expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.secondary.http-address", expectedHostName + ":" + expectedPortNum);
<     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", expectedHostName + ":" + expectedPortNum);
< 
<     coreSiteProperties.put("fs.default.name", expectedHostName + ":" + expectedPortNum);
<     coreSiteProperties.put("fs.defaultFS", "hdfs:
---
>     updatedVal = updatedVal.replaceAll("[\\[\\]]", "");
2814c2755
<     hbaseSiteProperties.put("hbase.rootdir", "hdfs:
---
>     String[] hosts = updatedVal.split(",");
2816c2757,2761
<     accumuloSiteProperties.put("instance.volumes", "hdfs:
---
>     Collection<String> expectedHosts = new HashSet<String>();
>     expectedHosts.add("'testhost:9090'");
>     expectedHosts.add("'testhost2:9091'");
>     expectedHosts.add("'testhost2a:9091'");
>     expectedHosts.add("'testhost2b:9091'");
2818,2819c2763,2768
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     assertEquals(4, hosts.length);
>     for (String host : hosts) {
>       assertTrue(expectedHosts.contains(host));
>       expectedHosts.remove(host);
>     }
>   }
2821,2822c2770,2778
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
---
>   @Test
>   public void testDoUpdateForClusterCreate_DBHostProperty__defaultValue() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hiveSiteProps = new HashMap<String, String>();
>     hiveSiteProps.put("javax.jdo.option.ConnectionURL", "jdbc:mysql:
>     Map<String, String> hiveEnvProps = new HashMap<String, String>();
>     hiveEnvProps.put("hive_database", "New MySQL Database");
>     properties.put("hive-site", hiveSiteProps);
>     properties.put("hive-env", hiveEnvProps);
2824,2837c2780
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.http.address"));
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.https.address"));
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.http-address"));
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.https-address"));
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.secondary.http.address"));
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.secondary.http-address"));
<     assertEquals("hdfs config property not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2839,2842c2782,2787
<     assertEquals("hdfs config in core-site not exported properly",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), coreSiteProperties.get("fs.default.name"));
<     assertEquals("hdfs config in core-site not exported properly",
<       "hdfs:
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     hgComponents.add("MYSQL_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2844,2845c2789,2792
<     assertEquals("hdfs config in hbase-site not exported properly",
<       "hdfs:
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2847,2848c2794,2796
<     assertEquals("hdfs config in accumulo-site not exported properly",
<       "hdfs:
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2850c2798,2799
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2851a2801,2803
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hive-site").get("javax.jdo.option.ConnectionURL");
>     assertEquals("jdbc:mysql:
2855,2862c2807,2814
<   public void testHDFSConfigClusterUpdateQuorumJournalURL() throws Exception {
<     final String expectedHostNameOne = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.apache.ambari.org";
<     final String expectedPortNum = "808080";
<     final String expectedHostGroupName = "host_group_1";
<     final String expectedHostGroupNameTwo = "host_group_2";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>   public void testDoUpdateForClusterCreate_DBHostProperty__exportedValue() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hiveSiteProps = new HashMap<String, String>();
>     hiveSiteProps.put("javax.jdo.option.ConnectionURL", "jdbc:mysql:
>     Map<String, String> hiveEnvProps = new HashMap<String, String>();
>     hiveEnvProps.put("hive_database", "New MySQL Database");
>     properties.put("hive-site", hiveSiteProps);
>     properties.put("hive-env", hiveEnvProps);
2864,2865c2816
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2867,2868c2818,2823
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostNameOne)).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo)).atLeastOnce();
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     hgComponents.add("MYSQL_SERVER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2870c2825,2828
<     mockSupport.replayAll();
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2872,2873c2830,2832
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2875,2876c2834,2835
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2878c2837,2840
<     configProperties.put("hdfs-site", hdfsSiteProperties);
---
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hive-site").get("javax.jdo.option.ConnectionURL");
>     assertEquals("jdbc:mysql:
>   }
2880,2882c2842,2848
<     
<     
<     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", "qjournal:
---
>   @Test
>   public void testDoUpdateForClusterCreate_DBHostProperty__external() throws Exception {
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> typeProps = new HashMap<String, String>();
>     typeProps.put("javax.jdo.option.ConnectionURL", "jdbc:mysql:
>     typeProps.put("hive_database", "Existing MySQL Database");
>     properties.put("hive-env", typeProps);
2884,2885c2850
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2887,2890c2852,2856
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put(expectedHostGroupNameTwo, mockHostGroupTwo);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     hgComponents.add("SECONDARY_NAMENODE");
>     hgComponents.add("RESOURCEMANAGER");
>     TestHostGroup group1 = new TestHostGroup("group1", hgComponents, Collections.singleton("testhost"));
2892,2893c2858,2861
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("DATANODE");
>     hgComponents2.add("HDFS_CLIENT");
>     TestHostGroup group2 = new TestHostGroup("group2", hgComponents2, Collections.singleton("testhost2"));
2895,2898c2863,2865
<     
<     assertEquals("HDFS HA shared edits directory property not properly updated for cluster create.",
<       "qjournal:
<       hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
2900c2867,2868
<     mockSupport.verifyAll();
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2901a2870,2872
>     updater.doUpdateForClusterCreate();
>     String updatedVal = topology.getConfiguration().getFullProperties().get("hive-env").get("javax.jdo.option.ConnectionURL");
>     assertEquals("jdbc:mysql:
2905,2907c2876,2877
<   public void testHDFSConfigClusterUpdateQuorumJournalURLSpecifyingHostNamesDirectly() throws Exception {
<     final String expectedHostNameOne = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.apache.ambari.org";
---
>   public void testFalconConfigClusterUpdate() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
2910,2917d2879
<     final String expectedHostGroupNameTwo = "host_group_2";
<     final String expectedQuorumJournalURL = "qjournal:
<                                             createHostAddress(expectedHostNameTwo, expectedPortNum) + "/mycluster";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
2919d2880
<     mockSupport.replayAll();
2921,2927c2882,2884
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
< 
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
< 
<     configProperties.put("hdfs-site", hdfsSiteProperties);
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> falconStartupProperties = new HashMap<String, String>();
>     properties.put("falcon-startup.properties", falconStartupProperties);
2930c2887,2889
<     
---
>     falconStartupProperties.put("*.broker.url", createExportedAddress(expectedPortNum, expectedHostGroupName));
>     falconStartupProperties.put("*.falcon.service.authentication.kerberos.principal", "falcon/" + createExportedHostName(expectedHostGroupName) + "@EXAMPLE.COM");
>     falconStartupProperties.put("*.falcon.http.authentication.kerberos.principal", "HTTP/" + createExportedHostName(expectedHostGroupName) + "@EXAMPLE.COM");
2932c2891
<     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", expectedQuorumJournalURL);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2934,2935c2893,2899
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("FALCON_SERVER");
>     hgComponents.add("FALCON_CLIENT");
>     List<String> hosts = new ArrayList<String>();
>     hosts.add("c6401.apache.ambari.org");
>     hosts.add("serverTwo");
>     TestHostGroup group1 = new TestHostGroup("host_group_1", hgComponents, hosts);
2937,2940c2901,2902
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put(expectedHostGroupNameTwo, mockHostGroupTwo);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
2942,2943c2904,2905
<     
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2945,2948c2907,2910
<     
<     assertEquals("HDFS HA shared edits directory property should not have been modified, since FQDNs were specified.",
<       expectedQuorumJournalURL,
<       hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
---
>     updater.doUpdateForClusterCreate();
> 
>     assertEquals("Falcon Broker URL property not properly exported",
>       expectedHostName + ":" + expectedPortNum, falconStartupProperties.get("*.broker.url"));
2950c2912,2913
<     mockSupport.verifyAll();
---
>     assertEquals("Falcon Kerberos Principal property not properly exported",
>       "falcon/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.service.authentication.kerberos.principal"));
2951a2915,2916
>     assertEquals("Falcon Kerberos HTTP Principal property not properly exported",
>       "HTTP/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.http.authentication.kerberos.principal"));
2955,2957c2920,2924
<   public void testHDFSConfigClusterUpdateQuorumJournalURL_UsingMinusSymbolInHostName() throws Exception {
<     final String expectedHostNameOne = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.apache.ambari.org";
---
>   
>   
>   
>   public void testFalconConfigClusterUpdateDefaultConfig() throws Exception {
>     final String expectedHostName = "c6401.apache.ambari.org";
2959,2970c2926
<     final String expectedHostGroupName = "host-group-1";
<     final String expectedHostGroupNameTwo = "host-group-2";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostNameOne)).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo)).atLeastOnce();
< 
<     mockSupport.replayAll();
---
>     final String expectedHostGroupName = "host_group_1";
2972,2973c2928,2930
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> falconStartupProperties = new HashMap<String, String>();
>     properties.put("falcon-startup.properties", falconStartupProperties);
2975,2976c2932,2935
<     Map<String, String> hdfsSiteProperties =
<       new HashMap<String, String>();
---
>     
>     falconStartupProperties.put("*.broker.url", "localhost:" + expectedPortNum);
>     falconStartupProperties.put("*.falcon.service.authentication.kerberos.principal", "falcon/" + "localhost" + "@EXAMPLE.COM");
>     falconStartupProperties.put("*.falcon.http.authentication.kerberos.principal", "HTTP/" + "localhost" + "@EXAMPLE.COM");
2978c2937
<     configProperties.put("hdfs-site", hdfsSiteProperties);
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
2980,2982c2939,2945
<     
<     
<     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", "qjournal:
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("FALCON_SERVER");
>     hgComponents.add("FALCON_CLIENT");
>     List<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add("serverTwo");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, hosts);
2984,2985c2947,2948
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
2987,2990c2950,2951
<     Map<String, HostGroup> mapOfHostGroups =
<       new HashMap<String, HostGroup>();
<     mapOfHostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     mapOfHostGroups.put(expectedHostGroupNameTwo, mockHostGroupTwo);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
2993c2954
<     configProcessor.doUpdateForClusterCreate(mapOfHostGroups, null);
---
>     updater.doUpdateForClusterCreate();
2995,2998c2956,2957
<     
<     assertEquals("HDFS HA shared edits directory property not properly updated for cluster create.",
<       "qjournal:
<       hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
---
>     assertEquals("Falcon Broker URL property not properly exported",
>       expectedHostName + ":" + expectedPortNum, falconStartupProperties.get("*.broker.url"));
3000c2959,2960
<     mockSupport.verifyAll();
---
>     assertEquals("Falcon Kerberos Principal property not properly exported",
>       "falcon/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.service.authentication.kerberos.principal"));
3001a2962,2963
>     assertEquals("Falcon Kerberos HTTP Principal property not properly exported",
>       "HTTP/" + expectedHostName + "@EXAMPLE.COM", falconStartupProperties.get("*.falcon.http.authentication.kerberos.principal"));
3005,3008c2967
<   public void testHiveConfigExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.ambari.apache.org";
<     final String expectedPortNum = "808080";
---
>   public void testHiveConfigClusterUpdateCustomValue() throws Exception {
3010d2968
<     final String expectedHostGroupNameTwo = "host_group_2";
3012,3020c2970,2971
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
<     expect(mockHostGroupTwo.getName()).andReturn(expectedHostGroupNameTwo).atLeastOnce();
---
>     final String expectedPropertyValue =
>       "hive.metastore.local=false,hive.metastore.uris=thrift:
3022c2973,2975
<     mockSupport.replayAll();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> webHCatSiteProperties = new HashMap<String, String>();
>     properties.put("webhcat-site", webHCatSiteProperties);
3024,3025c2977,2978
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     
>     webHCatSiteProperties.put("templeton.hive.properties", expectedPropertyValue);
3027,3034c2980
<     Map<String, String> hiveSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hiveEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3036,3039c2982,2986
<     configProperties.put("hive-site", hiveSiteProperties);
<     configProperties.put("hive-env", hiveEnvProperties);
<     configProperties.put("webhcat-site", webHCatSiteProperties);
<     configProperties.put("core-site", coreSiteProperties);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     List<String> hosts = new ArrayList<String>();
>     hosts.add("some-hose");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, hosts);
3042,3047c2989,2990
<     
<     hiveSiteProperties.put("hive.metastore.uris", "thrift:
<     hiveSiteProperties.put("javax.jdo.option.ConnectionURL", expectedHostName + ":" + expectedPortNum);
<     hiveSiteProperties.put("hive.zookeeper.quorum", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
<     hiveSiteProperties.put("hive.cluster.delegation.token.store.zookeeper.connectString", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
<     hiveEnvProperties.put("hive_hostname", expectedHostName);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
3048a2992,2993
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3050,3051c2995,2996
<     webHCatSiteProperties.put("templeton.hive.properties", expectedHostName + "," + expectedHostNameTwo);
<     webHCatSiteProperties.put("templeton.kerberos.principal", expectedHostName);
---
>     
>     updater.doUpdateForClusterCreate();
3053,3055c2998,3001
<     coreSiteProperties.put("hadoop.proxyuser.hive.hosts", expectedHostName + "," + expectedHostNameTwo);
<     coreSiteProperties.put("hadoop.proxyuser.HTTP.hosts", expectedHostName + "," + expectedHostNameTwo);
<     coreSiteProperties.put("hadoop.proxyuser.hcat.hosts", expectedHostName + "," + expectedHostNameTwo);
---
>     assertEquals("Unexpected config update for templeton.hive.properties",
>       expectedPropertyValue,
>       webHCatSiteProperties.get("templeton.hive.properties"));
>   }
3057,3058c3003,3006
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>   @Test
>   public void testHiveConfigClusterUpdateDefaultValue() throws Exception {
>     final String expectedHostGroupName = "host_group_1";
>     final String expectedHostName = "c6401.ambari.apache.org";
3060,3061c3008,3009
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne, mockHostGroupTwo));
---
>     final String expectedPropertyValue =
>       "hive.metastore.local=false,hive.metastore.uris=thrift:
3063,3068c3011,3013
<     assertEquals("hive property not properly exported",
<       "thrift:
<     assertEquals("hive property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hiveSiteProperties.get("javax.jdo.option.ConnectionURL"));
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName), hiveEnvProperties.get("hive_hostname"));
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> webHCatSiteProperties = new HashMap<String, String>();
>     properties.put("webhcat-site", webHCatSiteProperties);
3070,3074c3015,3017
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       webHCatSiteProperties.get("templeton.hive.properties"));
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName), webHCatSiteProperties.get("templeton.kerberos.principal"));
---
>     
>     webHCatSiteProperties.put("templeton.hive.properties",
>       expectedPropertyValue);
3076,3077c3019
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hive.hosts"));
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3079,3080c3021,3025
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.HTTP.hosts"));
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("HIVE_METASTORE");
>     List<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, hosts);
3082,3083c3027,3028
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hcat.hosts"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
3085,3087c3030,3031
<     assertEquals("hive zookeeper quorum property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
<       hiveSiteProperties.get("hive.zookeeper.quorum"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3089,3091c3033,3034
<     assertEquals("hive zookeeper connectString property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
<       hiveSiteProperties.get("hive.cluster.delegation.token.store.zookeeper.connectString"));
---
>     
>     updater.doUpdateForClusterCreate();
3093c3036,3039
<     mockSupport.verifyAll();
---
>     
>     assertEquals("Unexpected config update for templeton.hive.properties",
>       "hive.metastore.local=false,hive.metastore.uris=thrift:
>       webHCatSiteProperties.get("templeton.hive.properties"));
3097,3100c3043
<   public void testHiveConfigExportedMultipleHiveMetaStoreServers() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.ambari.apache.org";
<     final String expectedPortNum = "808080";
---
>   public void testHiveConfigClusterUpdateExportedHostGroupValue() throws Exception {
3102c3045
<     final String expectedHostGroupNameTwo = "host_group_2";
---
>     final String expectedHostName = "c6401.ambari.apache.org";
3104c3047,3049
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     
>     final String expectedPropertyValue =
>       "hive.metastore.local=false,hive.metastore.uris=thrift:
3106,3107d3050
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
3109,3112c3052,3054
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
<     expect(mockHostGroupTwo.getName()).andReturn(expectedHostGroupNameTwo).atLeastOnce();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> webHCatSiteProperties = new HashMap<String, String>();
>     properties.put("webhcat-site", webHCatSiteProperties);
3114c3056,3058
<     mockSupport.replayAll();
---
>     
>     webHCatSiteProperties.put("templeton.hive.properties",
>       expectedPropertyValue);
3116,3117c3060
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3119,3126c3062,3066
<     Map<String, String> hiveSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hiveEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("HIVE_METASTORE");
>     List<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, hosts);
3128,3131c3068,3069
<     configProperties.put("hive-site", hiveSiteProperties);
<     configProperties.put("hive-env", hiveEnvProperties);
<     configProperties.put("webhcat-site", webHCatSiteProperties);
<     configProperties.put("core-site", coreSiteProperties);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
3132a3071,3072
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3135,3139c3075
<     hiveSiteProperties.put("hive.metastore.uris", "thrift:
<     hiveSiteProperties.put("javax.jdo.option.ConnectionURL", expectedHostName + ":" + expectedPortNum);
<     hiveSiteProperties.put("hive.zookeeper.quorum", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
<     hiveSiteProperties.put("hive.cluster.delegation.token.store.zookeeper.connectString", expectedHostName + ":" + expectedPortNum + "," + expectedHostNameTwo + ":" + expectedPortNum);
<     hiveEnvProperties.put("hive_hostname", expectedHostName);
---
>     updater.doUpdateForClusterCreate();
3140a3077,3081
>     
>     assertEquals("Unexpected config update for templeton.hive.properties",
>       "hive.metastore.local=false,hive.metastore.uris=thrift:
>       webHCatSiteProperties.get("templeton.hive.properties"));
>   }
3142,3143c3083,3085
<     webHCatSiteProperties.put("templeton.hive.properties", expectedHostName + "," + expectedHostNameTwo);
<     webHCatSiteProperties.put("templeton.kerberos.principal", expectedHostName);
---
>   @Test
>   public void testStormAndKafkaConfigClusterUpdateWithoutGangliaServer() throws Exception {
>     final String expectedHostGroupName = "host_group_1";
3145,3147c3087,3089
<     coreSiteProperties.put("hadoop.proxyuser.hive.hosts", expectedHostName + "," + expectedHostNameTwo);
<     coreSiteProperties.put("hadoop.proxyuser.HTTP.hosts", expectedHostName + "," + expectedHostNameTwo);
<     coreSiteProperties.put("hadoop.proxyuser.hcat.hosts", expectedHostName + "," + expectedHostNameTwo);
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> stormSiteProperties = new HashMap<String, String>();
>     Map<String, String> kafkaBrokerProperties = new HashMap<String, String>();
3149,3150c3091,3092
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     properties.put("storm-site", stormSiteProperties);
>     properties.put("kafka-broker", kafkaBrokerProperties);
3152,3153c3094,3096
<     
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne, mockHostGroupTwo));
---
>     stormSiteProperties.put("worker.childopts", "localhost");
>     stormSiteProperties.put("supervisor.childopts", "localhost");
>     stormSiteProperties.put("nimbus.childopts", "localhost");
3154a3098
>     kafkaBrokerProperties.put("kafka.ganglia.metrics.host", "localhost");
3156c3100
<     System.out.println("RWN: exported value of hive.metastore.uris = " + hiveSiteProperties.get("hive.metastore.uris"));
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3158,3163c3102,3104
<     assertEquals("hive property not properly exported",
<       "thrift:
<     assertEquals("hive property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName), hiveSiteProperties.get("javax.jdo.option.ConnectionURL"));
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName), hiveEnvProperties.get("hive_hostname"));
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("HIVE_METASTORE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton("testserver"));
3165,3169c3106,3107
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       webHCatSiteProperties.get("templeton.hive.properties"));
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName), webHCatSiteProperties.get("templeton.kerberos.principal"));
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
3171,3172c3109
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hive.hosts"));
---
>     expect(stack.getCardinality("GANGLIA_SERVER")).andReturn(new Cardinality("1")).anyTimes();
3174,3175c3111,3112
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.HTTP.hosts"));
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3177,3178c3114,3115
<     assertEquals("hive property not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.hcat.hosts"));
---
>     
>     updater.doUpdateForClusterCreate();
3180,3182c3117,3120
<     assertEquals("hive zookeeper quorum property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
<       hiveSiteProperties.get("hive.zookeeper.quorum"));
---
>     
>     
>     assertEquals("worker startup settings not properly handled by cluster create",
>       "localhost", stormSiteProperties.get("worker.childopts"));
3184,3186c3122,3123
<     assertEquals("hive zookeeper connectString property not properly exported",
<       createExportedAddress(expectedPortNum, expectedHostGroupName) + "," + createExportedAddress(expectedPortNum, expectedHostGroupNameTwo),
<       hiveSiteProperties.get("hive.cluster.delegation.token.store.zookeeper.connectString"));
---
>     assertEquals("supervisor startup settings not properly handled by cluster create",
>       "localhost", stormSiteProperties.get("supervisor.childopts"));
3188,3189c3125,3126
<     mockSupport.verifyAll();
<   }
---
>     assertEquals("nimbus startup settings not properly handled by cluster create",
>       "localhost", stormSiteProperties.get("nimbus.childopts"));
3190a3128,3130
>     assertEquals("Kafka ganglia host property not properly handled by cluster create",
>       "localhost", kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
>   }
3193c3133
<   public void testOozieConfigExported() throws Exception {
---
>   public void testStormandKafkaConfigClusterUpdateWithGangliaServer() throws Exception {
3195,3196d3134
<     final String expectedHostNameTwo = "c6402.ambari.apache.org";
<     final String expectedExternalHost = "c6408.ambari.apache.org";
3198d3135
<     final String expectedHostGroupNameTwo = "host_group_2";
3200,3210c3137,3139
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
<     expect(mockHostGroupTwo.getName()).andReturn(expectedHostGroupNameTwo).atLeastOnce();
< 
<     mockSupport.replayAll();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> stormSiteProperties = new HashMap<String, String>();
>     Map<String, String> kafkaBrokerProperties = new HashMap<String, String>();
3212,3213c3141,3142
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     properties.put("storm-site", stormSiteProperties);
>     properties.put("kafka-broker", kafkaBrokerProperties);
3215,3220c3144,3146
<     Map<String, String> oozieSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> oozieEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
---
>     stormSiteProperties.put("worker.childopts", "localhost");
>     stormSiteProperties.put("supervisor.childopts", "localhost");
>     stormSiteProperties.put("nimbus.childopts", "localhost");
3222,3225c3148
<     configProperties.put("oozie-site", oozieSiteProperties);
<     configProperties.put("oozie-env", oozieEnvProperties);
<     configProperties.put("hive-env", oozieEnvProperties);
<     configProperties.put("core-site", coreSiteProperties);
---
>     kafkaBrokerProperties.put("kafka.ganglia.metrics.host", "localhost");
3227,3230c3150
<     oozieSiteProperties.put("oozie.base.url", expectedHostName);
<     oozieSiteProperties.put("oozie.authentication.kerberos.principal", expectedHostName);
<     oozieSiteProperties.put("oozie.service.HadoopAccessorService.kerberos.principal", expectedHostName);
<     oozieSiteProperties.put("oozie.service.JPAService.jdbc.url", "jdbc:mysql:
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3232,3233c3152,3154
<     oozieEnvProperties.put("oozie_hostname", expectedHostName);
<     oozieEnvProperties.put("oozie_existing_mysql_host", expectedExternalHost);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("GANGLIA_SERVER");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton(expectedHostName));
3235c3156,3157
<     coreSiteProperties.put("hadoop.proxyuser.oozie.hosts", expectedHostName + "," + expectedHostNameTwo);
---
>     Collection<TestHostGroup> hostGroups = new HashSet<TestHostGroup>();
>     hostGroups.add(group1);
3237,3238c3159,3160
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3241,3252c3163
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne, mockHostGroupTwo));
< 
<     assertEquals("oozie property not exported correctly",
<       createExportedHostName(expectedHostGroupName), oozieSiteProperties.get("oozie.base.url"));
<     assertEquals("oozie property not exported correctly",
<       createExportedHostName(expectedHostGroupName), oozieSiteProperties.get("oozie.authentication.kerberos.principal"));
<     assertEquals("oozie property not exported correctly",
<       createExportedHostName(expectedHostGroupName), oozieSiteProperties.get("oozie.service.HadoopAccessorService.kerberos.principal"));
<     assertEquals("oozie property not exported correctly",
<       createExportedHostName(expectedHostGroupName), oozieEnvProperties.get("oozie_hostname"));
<     assertEquals("oozie property not exported correctly",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo), coreSiteProperties.get("hadoop.proxyuser.oozie.hosts"));
---
>     updater.doUpdateForClusterCreate();
3255,3258c3166,3171
<     assertFalse("oozie_existing_mysql_host should not have been present in the exported configuration",
<       oozieEnvProperties.containsKey("oozie_existing_mysql_host"));
<     assertFalse("oozie.service.JPAService.jdbc.url should not have been present in the exported configuration",
<       oozieSiteProperties.containsKey("oozie.service.JPAService.jdbc.url"));
---
>     
>     assertEquals("worker startup settings not properly handled by cluster create",
>       expectedHostName, stormSiteProperties.get("worker.childopts"));
> 
>     assertEquals("supervisor startup settings not properly handled by cluster create",
>       expectedHostName, stormSiteProperties.get("supervisor.childopts"));
3260c3173,3174
<     mockSupport.verifyAll();
---
>     assertEquals("nimbus startup settings not properly handled by cluster create",
>       expectedHostName, stormSiteProperties.get("nimbus.childopts"));
3261a3176,3177
>     assertEquals("Kafka ganglia host property not properly handled by cluster create",
>       expectedHostName, kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
3265c3181,3182
<   public void testOozieConfigClusterUpdateHAEnabledSpecifyingHostNamesDirectly() throws Exception {
---
>   public void testDoUpdateForClusterWithNameNodeHAEnabled() throws Exception {
>     final String expectedNameService = "mynameservice";
3267,3268c3184,3187
<     final String expectedHostNameTwo = "c6402.ambari.apache.org";
<     final String expectedExternalHost = "c6408.ambari.apache.org";
---
>     final String expectedHostNameTwo = "serverTwo";
>     final String expectedPortNum = "808080";
>     final String expectedNodeOne = "nn1";
>     final String expectedNodeTwo = "nn2";
3270d3188
<     final String expectedHostGroupNameTwo = "host_group_2";
3272c3190,3206
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
> 
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     Map<String, String> hbaseSiteProperties = new HashMap<String, String>();
>     Map<String, String> hadoopEnvProperties = new HashMap<String, String>();
>     Map<String, String> coreSiteProperties = new HashMap<String, String>();
>     Map<String, String> accumuloSiteProperties =new HashMap<String, String>();
> 
>     properties.put("hdfs-site", hdfsSiteProperties);
>     properties.put("hadoop-env", hadoopEnvProperties);
>     properties.put("core-site", coreSiteProperties);
>     properties.put("hbase-site", hbaseSiteProperties);
>     properties.put("accumulo-site", accumuloSiteProperties);
> 
>     
>     hdfsSiteProperties.put("dfs.nameservices", expectedNameService);
>     hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice", expectedNodeOne + ", " + expectedNodeTwo);
3274,3275c3208,3214
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
3277c3216,3219
<     Stack mockStack = mockSupport.createMock(Stack.class);
---
>     
>     
>     hdfsSiteProperties.put("dfs.secondary.http.address", "localhost:8080");
>     hdfsSiteProperties.put("dfs.namenode.secondary.http-address", "localhost:8080");
3279,3280c3221,3222
<     expect(mockHostGroupOne.getComponents()).andReturn(Collections.singleton("OOZIE_SERVER")).atLeastOnce();
<     expect(mockHostGroupTwo.getComponents()).andReturn(Collections.singleton("OOZIE_SERVER")).atLeastOnce();
---
>     
>     coreSiteProperties.put("fs.defaultFS", "hdfs:
3282c3224,3225
<     expect(mockStack.getCardinality("OOZIE_SERVER")).andReturn(new Cardinality("1+")).atLeastOnce();
---
>     
>     hbaseSiteProperties.put("hbase.rootdir", "hdfs:
3284c3227,3228
<     mockSupport.replayAll();
---
>     
>     accumuloSiteProperties.put("instance.volumes", "hdfs:
3286,3287c3230
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3289,3294c3232,3234
<     Map<String, String> oozieSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> oozieEnvProperties =
<       new HashMap<String, String>();
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents, Collections.singleton(expectedHostName));
3296,3299c3236,3238
<     configProperties.put("oozie-site", oozieSiteProperties);
<     configProperties.put("oozie-env", oozieEnvProperties);
<     configProperties.put("hive-env", oozieEnvProperties);
<     configProperties.put("core-site", coreSiteProperties);
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("NAMENODE");
>     TestHostGroup group2 = new TestHostGroup("host-group-2", hgComponents2, Collections.singleton(expectedHostNameTwo));
3301,3304c3240,3242
<     oozieSiteProperties.put("oozie.base.url", expectedHostName);
<     oozieSiteProperties.put("oozie.authentication.kerberos.principal", expectedHostName);
<     oozieSiteProperties.put("oozie.service.HadoopAccessorService.kerberos.principal", expectedHostName);
<     oozieSiteProperties.put("oozie.service.JPAService.jdbc.url", "jdbc:mysql:
---
>     Collection<TestHostGroup> hostGroups = new ArrayList<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
3306,3308c3244,3245
<     
<     oozieSiteProperties.put("oozie.services.ext",
<       "org.apache.oozie.service.ZKLocksService,org.apache.oozie.service.ZKXLogStreamingService,org.apache.oozie.service.ZKJobsConcurrencyService,org.apache.oozie.service.ZKUUIDService");
---
>     expect(stack.getCardinality("NAMENODE")).andReturn(new Cardinality("1-2")).anyTimes();
>     expect(stack.getCardinality("SECONDARY_NAMENODE")).andReturn(new Cardinality("1")).anyTimes();
3309a3247,3248
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3311,3312c3250
<     oozieEnvProperties.put("oozie_hostname", expectedHostName);
<     oozieEnvProperties.put("oozie_existing_mysql_host", expectedExternalHost);
---
>     updater.doUpdateForClusterCreate();
3314c3252,3256
<     coreSiteProperties.put("hadoop.proxyuser.oozie.hosts", expectedHostName + "," + expectedHostNameTwo);
---
>     
>     assertEquals("HTTPS address HA property not properly exported",
>       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
3316,3317c3258,3261
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     assertEquals("HTTPS address HA property not properly exported",
>       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
3319,3322c3263,3266
<     Map<String, HostGroup> hostGroups =
<       new HashMap<String, HostGroup>();
<     hostGroups.put(expectedHostGroupName, mockHostGroupOne);
<     hostGroups.put(expectedHostGroupNameTwo, mockHostGroupTwo);
---
>     assertEquals("HTTPS address HA property not properly exported",
>       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported",
>       expectedHostName + ":" + expectedPortNum, hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
3325c3269,3281
<     configProcessor.doUpdateForClusterCreate(hostGroups, mockStack);
---
>     
>     
>     String initialActiveHost = hadoopEnvProperties.get("dfs_ha_initial_namenode_active");
>     String expectedStandbyHost = null;
>     if (initialActiveHost.equals(expectedHostName)) {
>       expectedStandbyHost = expectedHostNameTwo;
>     } else if (initialActiveHost.equals(expectedHostNameTwo)) {
>       expectedStandbyHost = expectedHostName;
>     } else {
>       fail("Active Namenode hostname was not set correctly");
>     }
>     assertEquals("Standby Namenode hostname was not set correctly",
>       expectedStandbyHost, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
3327,3336d3282
<     assertEquals("oozie property not updated correctly",
<       expectedHostName, oozieSiteProperties.get("oozie.base.url"));
<     assertEquals("oozie property not updated correctly",
<       expectedHostName, oozieSiteProperties.get("oozie.authentication.kerberos.principal"));
<     assertEquals("oozie property not updated correctly",
<       expectedHostName, oozieSiteProperties.get("oozie.service.HadoopAccessorService.kerberos.principal"));
<     assertEquals("oozie property not updated correctly",
<       expectedHostName, oozieEnvProperties.get("oozie_hostname"));
<     assertEquals("oozie property not updated correctly",
<       expectedHostName + "," + expectedHostNameTwo, coreSiteProperties.get("hadoop.proxyuser.oozie.hosts"));
3338c3284,3288
<     mockSupport.verifyAll();
---
>     assertEquals("fs.defaultFS should not be modified by cluster update when NameNode HA is enabled.",
>                  "hdfs:
> 
>     assertEquals("hbase.rootdir should not be modified by cluster update when NameNode HA is enabled.",
>       "hdfs:
3339a3290,3291
>     assertEquals("instance.volumes should not be modified by cluster update when NameNode HA is enabled.",
>         "hdfs:
3343,3345c3295,3301
<   public void testZookeeperConfigExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.ambari.apache.org";
---
>   public void testDoUpdateForClusterWithNameNodeHAEnabledAndActiveNodeSet() throws Exception {
>     final String expectedNameService = "mynameservice";
>     final String expectedHostName = "serverThree";
>     final String expectedHostNameTwo = "serverFour";
>     final String expectedPortNum = "808080";
>     final String expectedNodeOne = "nn1";
>     final String expectedNodeTwo = "nn2";
3347,3349d3302
<     final String expectedHostGroupNameTwo = "host_group_2";
<     final String expectedPortNumberOne = "2112";
<     final String expectedPortNumberTwo = "1221";
3351c3304,3306
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     Map<String, String> hadoopEnvProperties = new HashMap<String, String>();
3353,3354c3308,3309
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     properties.put("hdfs-site", hdfsSiteProperties);
>     properties.put("hadoop-env", hadoopEnvProperties);
3356,3359c3311,3313
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
<     expect(mockHostGroupTwo.getName()).andReturn(expectedHostGroupNameTwo).atLeastOnce();
---
>     
>     hdfsSiteProperties.put("dfs.nameservices", expectedNameService);
>     hdfsSiteProperties.put("dfs.ha.namenodes.mynameservice", expectedNodeOne + ", " + expectedNodeTwo);
3361c3315,3321
<     mockSupport.replayAll();
---
>     
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne, createExportedAddress(expectedPortNum, expectedHostGroupName));
>     hdfsSiteProperties.put("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo, createExportedAddress(expectedPortNum, expectedHostGroupName));
3363,3364c3323,3326
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     
>     
>     hadoopEnvProperties.put("dfs_ha_initial_namenode_active", expectedHostName);
>     hadoopEnvProperties.put("dfs_ha_initial_namenode_standby", expectedHostNameTwo);
3366,3379c3328
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> hbaseSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> sliderClientProperties =
<       new HashMap<String, String>();
<     Map<String, String> yarnSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> kafkaBrokerProperties =
<       new HashMap<String, String>();
<     Map<String, String> accumuloSiteProperties =
<         new HashMap<String, String>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3381,3387c3330,3335
<     configProperties.put("core-site", coreSiteProperties);
<     configProperties.put("hbase-site", hbaseSiteProperties);
<     configProperties.put("webhcat-site", webHCatSiteProperties);
<     configProperties.put("slider-client", sliderClientProperties);
<     configProperties.put("yarn-site", yarnSiteProperties);
<     configProperties.put("kafka-broker", kafkaBrokerProperties);
<     configProperties.put("accumulo-site", accumuloSiteProperties);
---
>     Collection<String> hgComponents = new HashSet<String>();
>     hgComponents.add("NAMENODE");
>     Collection<String> hosts = new ArrayList<String>();
>     hosts.add(expectedHostName);
>     hosts.add(expectedHostNameTwo);
>     TestHostGroup group = new TestHostGroup(expectedHostGroupName, hgComponents, hosts);
3389,3395c3337,3338
<     coreSiteProperties.put("ha.zookeeper.quorum", expectedHostName + "," + expectedHostNameTwo);
<     hbaseSiteProperties.put("hbase.zookeeper.quorum", expectedHostName + "," + expectedHostNameTwo);
<     webHCatSiteProperties.put("templeton.zookeeper.hosts", expectedHostName + "," + expectedHostNameTwo);
<     yarnSiteProperties.put("hadoop.registry.zk.quorum", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
<     sliderClientProperties.put("slider.zookeeper.quorum", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
<     kafkaBrokerProperties.put("zookeeper.connect", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
<     accumuloSiteProperties.put("instance.zookeeper.host", createHostAddress(expectedHostName, expectedPortNumberOne) + "," + createHostAddress(expectedHostNameTwo, expectedPortNumberTwo));
---
>     Collection<TestHostGroup> hostGroups = new ArrayList<TestHostGroup>();
>     hostGroups.add(group);
3396a3340,3341
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3398,3399c3343
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     updater.doUpdateForClusterCreate();
3402c3346,3354
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne, mockHostGroupTwo));
---
>     
>     
>     String expectedPropertyValue = hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeOne);
>     if (! expectedPropertyValue.equals(expectedHostName + ":" + expectedPortNum) &&
>         ! expectedPropertyValue.equals(expectedHostNameTwo + ":" + expectedPortNum)) {
>       fail("HTTPS address HA property not properly exported");
>     }
>     assertEquals("HTTPS address HA property not properly exported", expectedPropertyValue,
>         hdfsSiteProperties.get("dfs.namenode.https-address." + expectedNameService + "." + expectedNodeTwo));
3404,3424c3356,3364
<     assertEquals("zookeeper config not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       coreSiteProperties.get("ha.zookeeper.quorum"));
<     assertEquals("zookeeper config not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       hbaseSiteProperties.get("hbase.zookeeper.quorum"));
<     assertEquals("zookeeper config not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       webHCatSiteProperties.get("templeton.zookeeper.hosts"));
<     assertEquals("yarn-site zookeeper config not properly exported",
<       createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
<       yarnSiteProperties.get("hadoop.registry.zk.quorum"));
<     assertEquals("slider-client zookeeper config not properly exported",
<       createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
<       sliderClientProperties.get("slider.zookeeper.quorum"));
<     assertEquals("kafka zookeeper config not properly exported",
<       createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
<       kafkaBrokerProperties.get("zookeeper.connect"));
<     assertEquals("accumulo-site zookeeper config not properly exported",
<         createExportedHostName(expectedHostGroupName, expectedPortNumberOne) + "," + createExportedHostName(expectedHostGroupNameTwo, expectedPortNumberTwo),
<         accumuloSiteProperties.get("instance.zookeeper.host"));
---
>     assertEquals("HTTPS address HA property not properly exported", expectedPropertyValue,
>         hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported", expectedPropertyValue,
>         hdfsSiteProperties.get("dfs.namenode.http-address." + expectedNameService + "." + expectedNodeTwo));
> 
>     assertEquals("HTTPS address HA property not properly exported", expectedPropertyValue,
>         hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeOne));
>     assertEquals("HTTPS address HA property not properly exported", expectedPropertyValue,
>         hdfsSiteProperties.get("dfs.namenode.rpc-address." + expectedNameService + "." + expectedNodeTwo));
3426c3366,3370
<     mockSupport.verifyAll();
---
>     
>     
>     
>     assertEquals("Active Namenode hostname was not set correctly",
>       expectedHostName, hadoopEnvProperties.get("dfs_ha_initial_namenode_active"));
3427a3372,3373
>     assertEquals("Standby Namenode hostname was not set correctly",
>       expectedHostNameTwo, hadoopEnvProperties.get("dfs_ha_initial_namenode_standby"));
3431,3435c3377,3380
<   public void testKnoxSecurityConfigExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostNameTwo = "c6402.ambari.apache.org";
<     final String expectedHostGroupName = "host_group_1";
<     final String expectedHostGroupNameTwo = "host_group_2";
---
>   public void testParseNameServices() throws Exception {
>     Map<String, String> hdfsSiteConfigMap =
>       new HashMap<String, String>();
>     hdfsSiteConfigMap.put("dfs.nameservices", "serviceOne");
3437c3382,3383
<     EasyMockSupport mockSupport = new EasyMockSupport();
---
>     
>     String[] result = BlueprintConfigurationProcessor.parseNameServices(hdfsSiteConfigMap);
3439,3440c3385,3390
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
---
>     assertNotNull("Resulting array was null",
>       result);
>     assertEquals("Incorrect array size",
>       1, result.length);
>     assertEquals("Incorrect value for returned name service",
>       "serviceOne", result[0]);
3442,3445c3392,3393
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupTwo.getHostInfo()).andReturn(Arrays.asList(expectedHostNameTwo, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
<     expect(mockHostGroupTwo.getName()).andReturn(expectedHostGroupNameTwo).atLeastOnce();
---
>     
>     hdfsSiteConfigMap.put("dfs.nameservices", " serviceTwo, serviceThree, serviceFour");
3447c3395
<     mockSupport.replayAll();
---
>     String[] resultTwo = BlueprintConfigurationProcessor.parseNameServices(hdfsSiteConfigMap);
3449,3450c3397,3407
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     assertNotNull("Resulting array was null",
>       resultTwo);
>     assertEquals("Incorrect array size",
>       3, resultTwo.length);
>     assertEquals("Incorrect value for returned name service",
>       "serviceTwo", resultTwo[0]);
>     assertEquals("Incorrect value for returned name service",
>       "serviceThree", resultTwo[1]);
>     assertEquals("Incorrect value for returned name service",
>       "serviceFour", resultTwo[2]);
>   }
3452,3456c3409,3412
<     Map<String, String> coreSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> webHCatSiteProperties =
<       new HashMap<String, String>();
<     Map<String, String> oozieSiteProperties =
---
>   @Test
>   public void testParseNameNodes() throws Exception {
>     final String expectedServiceName = "serviceOne";
>     Map<String, String> hdfsSiteConfigMap =
3457a3414
>     hdfsSiteConfigMap.put("dfs.ha.namenodes." + expectedServiceName, "node1");
3459,3470c3416,3418
<     configProperties.put("core-site", coreSiteProperties);
<     configProperties.put("webhcat-site", webHCatSiteProperties);
<     configProperties.put("oozie-site", oozieSiteProperties);
< 
<     coreSiteProperties.put("hadoop.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
<     webHCatSiteProperties.put("webhcat.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
<     oozieSiteProperties.put("hadoop.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
<     oozieSiteProperties.put("oozie.service.ProxyUserService.proxyuser.knox.hosts", expectedHostName + "," + expectedHostNameTwo);
< 
< 
< 
< 
---
>     
>     String[] result =
>       BlueprintConfigurationProcessor.parseNameNodes(expectedServiceName, hdfsSiteConfigMap);
3472,3473c3420,3425
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     assertNotNull("Resulting array was null",
>       result);
>     assertEquals("Incorrect array size",
>       1, result.length);
>     assertEquals("Incorrect value for returned name nodes",
>       "node1", result[0]);
3476,3489c3428
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne, mockHostGroupTwo));
< 
<     assertEquals("Knox for core-site config not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       coreSiteProperties.get("hadoop.proxyuser.knox.hosts"));
<     assertEquals("Knox config for WebHCat not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       webHCatSiteProperties.get("webhcat.proxyuser.knox.hosts"));
<     assertEquals("Knox config for Oozie not properly exported",
<       createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<       oozieSiteProperties.get("hadoop.proxyuser.knox.hosts"));
<     assertEquals("Knox config for Oozie not properly exported",
<         createExportedHostName(expectedHostGroupName) + "," + createExportedHostName(expectedHostGroupNameTwo),
<         oozieSiteProperties.get("oozie.service.ProxyUserService.proxyuser.knox.hosts"));
---
>     hdfsSiteConfigMap.put("dfs.ha.namenodes." + expectedServiceName, " nodeSeven, nodeEight, nodeNine");
3491c3430,3431
<     mockSupport.verifyAll();
---
>     String[] resultTwo =
>       BlueprintConfigurationProcessor.parseNameNodes(expectedServiceName, hdfsSiteConfigMap);
3492a3433,3442
>     assertNotNull("Resulting array was null",
>       resultTwo);
>     assertEquals("Incorrect array size",
>       3, resultTwo.length);
>     assertEquals("Incorrect value for returned name node",
>       "nodeSeven", resultTwo[0]);
>     assertEquals("Incorrect value for returned name node",
>       "nodeEight", resultTwo[1]);
>     assertEquals("Incorrect value for returned name node",
>       "nodeNine", resultTwo[2]);
3496,3497c3446,3449
<   public void testKafkaConfigExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
---
>   public void testHDFSConfigClusterUpdateQuorumJournalURL() throws Exception {
>     final String expectedHostNameOne = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.apache.ambari.org";
>     final String expectedPortNum = "808080";
3499,3507c3451
<     final String expectedPortNumberOne = "2112";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
<     HostGroup mockHostGroupTwo = mockSupport.createMock(HostGroup.class);
< 
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
---
>     final String expectedHostGroupNameTwo = "host_group_2";
3509c3453,3455
<     mockSupport.replayAll();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     properties.put("hdfs-site", hdfsSiteProperties);
3511,3512c3457,3459
<     Map<String, Map<String, String>> configProperties =
<       new HashMap<String, Map<String, String>>();
---
>     
>     
>     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", "qjournal:
3514,3515c3461
<     Map<String, String> kafkaBrokerProperties =
<       new HashMap<String, String>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3517c3463,3465
<     configProperties.put("kafka-broker", kafkaBrokerProperties);
---
>     Collection<String> hgComponents1 = new HashSet<String>();
>     hgComponents1.add("NAMENODE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents1, Collections.singleton(expectedHostNameOne));
3519c3467,3469
<     kafkaBrokerProperties.put("kafka.ganglia.metrics.host", createHostAddress(expectedHostName, expectedPortNumberOne));
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("NAMENODE");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, hgComponents2, Collections.singleton(expectedHostNameTwo));
3520a3471,3473
>     Collection<TestHostGroup> hostGroups = new ArrayList<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
3522,3523c3475,3476
<     BlueprintConfigurationProcessor configProcessor =
<       new BlueprintConfigurationProcessor(configProperties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3526,3532c3479
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne, mockHostGroupTwo));
< 
<     assertEquals("kafka Ganglia config not properly exported",
<       createExportedHostName(expectedHostGroupName, expectedPortNumberOne),
<       kafkaBrokerProperties.get("kafka.ganglia.metrics.host"));
< 
<     mockSupport.verifyAll();
---
>     updater.doUpdateForClusterCreate();
3533a3481,3484
>     
>     assertEquals("HDFS HA shared edits directory property not properly updated for cluster create.",
>       "qjournal:
>       hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
3537,3543c3488,3493
<   public void testPropertyWithUndefinedHostisExported() throws Exception {
<     final String expectedHostName = "c6401.apache.ambari.org";
<     final String expectedHostGroupName = "host_group_1";
< 
<     EasyMockSupport mockSupport = new EasyMockSupport();
< 
<     HostGroup mockHostGroupOne = mockSupport.createMock(HostGroup.class);
---
>   public void testHDFSConfigClusterUpdateQuorumJournalURL_UsingMinusSymbolInHostName() throws Exception {
>     final String expectedHostNameOne = "c6401.apache.ambari.org";
>     final String expectedHostNameTwo = "c6402.apache.ambari.org";
>     final String expectedPortNum = "808080";
>     final String expectedHostGroupName = "host-group-1";
>     final String expectedHostGroupNameTwo = "host-group-2";
3545,3546c3495,3497
<     expect(mockHostGroupOne.getHostInfo()).andReturn(Arrays.asList(expectedHostName, "serverTwo")).atLeastOnce();
<     expect(mockHostGroupOne.getName()).andReturn(expectedHostGroupName).atLeastOnce();
---
>     Map<String, Map<String, String>> properties = new HashMap<String, Map<String, String>>();
>     Map<String, String> hdfsSiteProperties = new HashMap<String, String>();
>     properties.put("hdfs-site", hdfsSiteProperties);
3548c3499,3501
<     mockSupport.replayAll();
---
>     
>     
>     hdfsSiteProperties.put("dfs.namenode.shared.edits.dir", "qjournal:
3550c3503
<     Map<String, Map<String, String>> configProperties = new HashMap<String, Map<String, String>>();
---
>     Configuration clusterConfig = new Configuration(properties, Collections.<String, Map<String, Map<String, String>>>emptyMap());
3552,3553c3505,3507
<     Map<String, String> properties = new HashMap<String, String>();
<     configProperties.put("storm-site", properties);
---
>     Collection<String> hgComponents1 = new HashSet<String>();
>     hgComponents1.add("NAMENODE");
>     TestHostGroup group1 = new TestHostGroup(expectedHostGroupName, hgComponents1, Collections.singleton(expectedHostNameOne));
3555,3558c3509,3511
<     
<     properties.put("storm.zookeeper.servers", expectedHostName);
<     properties.put("nimbus.childopts", "undefined");
<     properties.put("worker.childopts", "some other info, undefined, more info");
---
>     Collection<String> hgComponents2 = new HashSet<String>();
>     hgComponents2.add("NAMENODE");
>     TestHostGroup group2 = new TestHostGroup(expectedHostGroupNameTwo, hgComponents2, Collections.singleton(expectedHostNameTwo));
3559a3513,3515
>     Collection<TestHostGroup> hostGroups = new ArrayList<TestHostGroup>();
>     hostGroups.add(group1);
>     hostGroups.add(group2);
3561,3562c3517,3518
<     BlueprintConfigurationProcessor configProcessor =
<         new BlueprintConfigurationProcessor(configProperties);
---
>     ClusterTopology topology = createClusterTopology("c1", bp, clusterConfig, hostGroups);
>     BlueprintConfigurationProcessor updater = new BlueprintConfigurationProcessor(topology);
3565,3572c3521
<     configProcessor.doUpdateForBlueprintExport(Arrays.asList(mockHostGroupOne));
< 
<     assertEquals("Property was incorrectly exported",
<         "%HOSTGROUP::" + expectedHostGroupName + "%", properties.get("storm.zookeeper.servers"));
<     assertEquals("Property with undefined host was incorrectly exported",
<         "undefined", properties.get("nimbus.childopts"));
<     assertEquals("Property with undefined host was incorrectly exported",
<         "some other info, undefined, more info" , properties.get("worker.childopts"));
---
>     updater.doUpdateForClusterCreate();
3574c3523,3526
<     mockSupport.verifyAll();
---
>     
>     assertEquals("HDFS HA shared edits directory property not properly updated for cluster create.",
>       "qjournal:
>       hdfsSiteProperties.get("dfs.namenode.shared.edits.dir"));
3577d3528
< 
3579c3530
<     return createExportedHostName(expectedHostGroupName) + ":" + expectedPortNum;
---
>     return createExportedHostName(expectedHostGroupName, expectedPortNum);
3595c3546,3548
<   private class TestHostGroup implements HostGroup {
---
>   private ClusterTopology createClusterTopology(String clusterName, Blueprint blueprint, Configuration configuration,
>                                                 Collection<TestHostGroup> hostGroups)
>       throws InvalidTopologyException {
3597,3599d3549
<     private String name;
<     private Collection<String> hosts;
<     private Collection<String> components;
3601,3605c3551
<     private TestHostGroup(String name, Collection<String> hosts, Collection<String> components) {
<       this.name = name;
<       this.hosts = hosts;
<       this.components = components;
<     }
---
>     replay(stack, serviceInfo);
3607,3610c3553,3567
<     @Override
<     public String getName() {
<       return name;
<     }
---
>     Map<String, HostGroupInfo> hostGroupInfo = new HashMap<String, HostGroupInfo>();
>     Collection<String> allServices = new HashSet<String>();
>     Map<String, HostGroup> allHostGroups = new HashMap<String, HostGroup>();
> 
>     for (TestHostGroup hostGroup : hostGroups) {
>       HostGroupInfo groupInfo = new HostGroupInfo(hostGroup.name);
>       groupInfo.addHosts(hostGroup.hosts);
>       
>       groupInfo.setConfiguration(EMPTY_CONFIG);
> 
>       
>       allHostGroups.put(hostGroup.name, new HostGroupImpl(hostGroup.name, "test-bp", stack,
>           hostGroup.components, EMPTY_CONFIG, "1"));
> 
>       hostGroupInfo.put(hostGroup.name, groupInfo);
3612,3614c3569,3575
<     @Override
<     public Collection<String> getHostInfo() {
<       return hosts;
---
>       for (String component : hostGroup.components) {
>         for (Map.Entry<String, Collection<String>> serviceComponentsEntry : serviceComponents.entrySet()) {
>           if (serviceComponentsEntry.getValue().contains(component)) {
>             allServices.add(serviceComponentsEntry.getKey());
>           }
>         }
>       }
3617,3619c3578,3581
<     @Override
<     public Collection<String> getComponents() {
<       return components;
---
>     expect(bp.getServices()).andReturn(allServices).anyTimes();
> 
>     for (HostGroup group : allHostGroups.values()) {
>       expect(bp.getHostGroup(group.getName())).andReturn(group).anyTimes();
3622,3624c3584,3599
<     @Override
<     public Map<String, Map<String, String>> getConfigurationProperties() {
<       return null;
---
>     expect(bp.getHostGroups()).andReturn(allHostGroups).anyTimes();
> 
>     replay(bp);
> 
>     return new ClusterTopologyImpl(clusterName, blueprint, configuration, hostGroupInfo);
>   }
> 
>   private class TestHostGroup {
>     private String name;
>     private Collection<String> components;
>     private Collection<String> hosts;
> 
>     public TestHostGroup(String name, Collection<String> components, Collection<String> hosts) {
>       this.name = name;
>       this.components = components;
>       this.hosts = hosts;
