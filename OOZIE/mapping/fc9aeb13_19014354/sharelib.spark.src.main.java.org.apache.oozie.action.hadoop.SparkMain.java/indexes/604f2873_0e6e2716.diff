23a24
> import org.apache.log4j.PropertyConfigurator;
26a28
> import java.io.FileOutputStream;
27a30,31
> import java.io.OutputStream;
> import java.net.URL;
30a35
> import java.util.Properties;
50a56,58
>     private static final String SPARK_LOG4J_PROPS = "spark-log4j.properties";
>     private static final Pattern[] SPARK_JOB_IDS_PATTERNS = {
>             Pattern.compile("Submitted application (application[0-9_]*)") };
61c69
< 
---
>         String logFile = setUpSparkLog4J(actionConf);
177a186,192
> 
>         sparkArgs.add("--conf");
>         sparkArgs.add("spark.executor.extraJavaOptions=-Dlog4j.configuration=" + SPARK_LOG4J_PROPS);
> 
>         sparkArgs.add("--conf");
>         sparkArgs.add("spark.driver.extraJavaOptions=-Dlog4j.configuration=" + SPARK_LOG4J_PROPS);
> 
207c222,227
<         runSpark(sparkArgs.toArray(new String[sparkArgs.size()]));
---
>         try {
>             runSpark(sparkArgs.toArray(new String[sparkArgs.size()]));
>         }
>         finally {
>             writeExternalChildIDs(logFile, SPARK_JOB_IDS_PATTERNS, "Spark");
>         }
333a354,398
> 
>     public static String setUpSparkLog4J(Configuration distcpConf) throws IOException {
>         
>         String hadoopJobId = System.getProperty("oozie.launcher.job.id");
>         if (hadoopJobId == null) {
>             throw new RuntimeException("Launcher Hadoop Job ID system,property not set");
>         }
>         String logFile = new File("spark-oozie-" + hadoopJobId + ".log").getAbsolutePath();
>         Properties hadoopProps = new Properties();
> 
>         
>         URL log4jFile = Thread.currentThread().getContextClassLoader().getResource("log4j.properties");
>         if (log4jFile != null) {
>             
>             hadoopProps.load(log4jFile.openStream());
>         }
> 
>         String logLevel = distcpConf.get("oozie.spark.log.level", "INFO");
>         String rootLogLevel = distcpConf.get("oozie.action." + LauncherMapper.ROOT_LOGGER_LEVEL, "INFO");
> 
>         hadoopProps.setProperty("log4j.rootLogger", rootLogLevel + ", A");
>         hadoopProps.setProperty("log4j.logger.org.apache.spark", logLevel + ", A, jobid");
>         hadoopProps.setProperty("log4j.additivity.org.apache.spark", "false");
>         hadoopProps.setProperty("log4j.appender.A", "org.apache.log4j.ConsoleAppender");
>         hadoopProps.setProperty("log4j.appender.A.layout", "org.apache.log4j.PatternLayout");
>         hadoopProps.setProperty("log4j.appender.A.layout.ConversionPattern", "%d [%t] %-5p %c %x - %m%n");
>         hadoopProps.setProperty("log4j.appender.jobid", "org.apache.log4j.FileAppender");
>         hadoopProps.setProperty("log4j.appender.jobid.file", logFile);
>         hadoopProps.setProperty("log4j.appender.jobid.layout", "org.apache.log4j.PatternLayout");
>         hadoopProps.setProperty("log4j.appender.jobid.layout.ConversionPattern", "%d [%t] %-5p %c %x - %m%n");
>         hadoopProps.setProperty("log4j.logger.org.apache.hadoop.mapred", "INFO, jobid");
>         hadoopProps.setProperty("log4j.logger.org.apache.hadoop.mapreduce.Job", "INFO, jobid");
>         hadoopProps.setProperty("log4j.logger.org.apache.hadoop.yarn.client.api.impl.YarnClientImpl", "INFO, jobid");
> 
>         String localProps = new File(SPARK_LOG4J_PROPS).getAbsolutePath();
>         OutputStream os1 = new FileOutputStream(localProps);
>         try {
>             hadoopProps.store(os1, "");
>         }
>         finally {
>             os1.close();
>         }
>         PropertyConfigurator.configure(SPARK_LOG4J_PROPS);
>         return logFile;
>     }
